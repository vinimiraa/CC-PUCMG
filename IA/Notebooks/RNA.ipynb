{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Vamos experimentar agora a Rede Neural Artificial?**\n",
        "Veja:\n",
        "https://scikit-learn.org/stable/modules/neural_networks_supervised.html# "
      ],
      "metadata": {
        "id": "_bPVTxb4akHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install yellowbrick"
      ],
      "metadata": {
        "id": "fpe0EYaXiIPm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ru9xg6QIaceV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mkdILomXTwn",
        "outputId": "e42868f6-592c-43b8-cbfa-bbea68845a8b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Arquivos CSV e PKL para google Colab/cancer.pkl', 'rb') as f:\n",
        "  X_treino, X_teste, y_treino, y_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "STeZ46Y4bKfl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vamos treinar com a rede neural?**\n",
        "\n",
        "**Experimente a RNA com os parâmetros default. A rede convergiu? quantas épocas?**"
      ],
      "metadata": {
        "id": "8bCxFBVNFt22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = MLPClassifier()\n",
        "#rede_neural = MLPClassifier(max_iter=1000, verbose=True, tol=0.00001, solver = 'adam', activation = 'relu',hidden_layer_sizes = (3, 2, 1))\n",
        "modelo.fit(X_treino, y_treino)"
      ],
      "metadata": {
        "id": "hVW22XucaswH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c08bca3-ed7c-4d92-c4a9-8030b70af513"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Depois execute novamente com os ajustes. Veja agora os erros a cada época.. estabeleça o verbose para true **"
      ],
      "metadata": {
        "id": "w6Yh14LUHfN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "modelo.fit(X_treino, y_treino)"
      ],
      "metadata": {
        "id": "6QRptikHHepQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0106c94-1301-44e9-be60-e108bf82bebb"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.72818496\n",
            "Iteration 2, loss = 0.69598631\n",
            "Iteration 3, loss = 0.66825764\n",
            "Iteration 4, loss = 0.64539323\n",
            "Iteration 5, loss = 0.62597860\n",
            "Iteration 6, loss = 0.60953516\n",
            "Iteration 7, loss = 0.59574679\n",
            "Iteration 8, loss = 0.58481498\n",
            "Iteration 9, loss = 0.57623941\n",
            "Iteration 10, loss = 0.56800979\n",
            "Iteration 11, loss = 0.56198147\n",
            "Iteration 12, loss = 0.55671190\n",
            "Iteration 13, loss = 0.55231047\n",
            "Iteration 14, loss = 0.54817643\n",
            "Iteration 15, loss = 0.54441508\n",
            "Iteration 16, loss = 0.54039756\n",
            "Iteration 17, loss = 0.53656374\n",
            "Iteration 18, loss = 0.53268038\n",
            "Iteration 19, loss = 0.52909724\n",
            "Iteration 20, loss = 0.52533059\n",
            "Iteration 21, loss = 0.52215972\n",
            "Iteration 22, loss = 0.51884408\n",
            "Iteration 23, loss = 0.51551125\n",
            "Iteration 24, loss = 0.51231253\n",
            "Iteration 25, loss = 0.50909862\n",
            "Iteration 26, loss = 0.50587870\n",
            "Iteration 27, loss = 0.50268818\n",
            "Iteration 28, loss = 0.50018347\n",
            "Iteration 29, loss = 0.49705312\n",
            "Iteration 30, loss = 0.49448681\n",
            "Iteration 31, loss = 0.49221054\n",
            "Iteration 32, loss = 0.48981345\n",
            "Iteration 33, loss = 0.48755832\n",
            "Iteration 34, loss = 0.48519736\n",
            "Iteration 35, loss = 0.48294261\n",
            "Iteration 36, loss = 0.48058862\n",
            "Iteration 37, loss = 0.47838069\n",
            "Iteration 38, loss = 0.47641858\n",
            "Iteration 39, loss = 0.47435923\n",
            "Iteration 40, loss = 0.47303542\n",
            "Iteration 41, loss = 0.47120693\n",
            "Iteration 42, loss = 0.46964666\n",
            "Iteration 43, loss = 0.46822394\n",
            "Iteration 44, loss = 0.46639549\n",
            "Iteration 45, loss = 0.46487367\n",
            "Iteration 46, loss = 0.46311727\n",
            "Iteration 47, loss = 0.46202140\n",
            "Iteration 48, loss = 0.46102605\n",
            "Iteration 49, loss = 0.45983083\n",
            "Iteration 50, loss = 0.45858403\n",
            "Iteration 51, loss = 0.45742327\n",
            "Iteration 52, loss = 0.45570932\n",
            "Iteration 53, loss = 0.45381773\n",
            "Iteration 54, loss = 0.45202011\n",
            "Iteration 55, loss = 0.44997385\n",
            "Iteration 56, loss = 0.44852930\n",
            "Iteration 57, loss = 0.44666369\n",
            "Iteration 58, loss = 0.44513026\n",
            "Iteration 59, loss = 0.44353055\n",
            "Iteration 60, loss = 0.44226860\n",
            "Iteration 61, loss = 0.44081801\n",
            "Iteration 62, loss = 0.43968646\n",
            "Iteration 63, loss = 0.43823902\n",
            "Iteration 64, loss = 0.43689048\n",
            "Iteration 65, loss = 0.43546994\n",
            "Iteration 66, loss = 0.43423772\n",
            "Iteration 67, loss = 0.43292339\n",
            "Iteration 68, loss = 0.43159815\n",
            "Iteration 69, loss = 0.43019062\n",
            "Iteration 70, loss = 0.42883095\n",
            "Iteration 71, loss = 0.42766181\n",
            "Iteration 72, loss = 0.42655429\n",
            "Iteration 73, loss = 0.42527256\n",
            "Iteration 74, loss = 0.42387424\n",
            "Iteration 75, loss = 0.42246424\n",
            "Iteration 76, loss = 0.42102959\n",
            "Iteration 77, loss = 0.41974950\n",
            "Iteration 78, loss = 0.41852206\n",
            "Iteration 79, loss = 0.41721290\n",
            "Iteration 80, loss = 0.41591530\n",
            "Iteration 81, loss = 0.41460684\n",
            "Iteration 82, loss = 0.41329548\n",
            "Iteration 83, loss = 0.41208382\n",
            "Iteration 84, loss = 0.41077955\n",
            "Iteration 85, loss = 0.40955113\n",
            "Iteration 86, loss = 0.40830093\n",
            "Iteration 87, loss = 0.40709920\n",
            "Iteration 88, loss = 0.40586555\n",
            "Iteration 89, loss = 0.40456224\n",
            "Iteration 90, loss = 0.40324187\n",
            "Iteration 91, loss = 0.40176943\n",
            "Iteration 92, loss = 0.40037451\n",
            "Iteration 93, loss = 0.39886350\n",
            "Iteration 94, loss = 0.39764441\n",
            "Iteration 95, loss = 0.39649722\n",
            "Iteration 96, loss = 0.39547714\n",
            "Iteration 97, loss = 0.39419092\n",
            "Iteration 98, loss = 0.39290241\n",
            "Iteration 99, loss = 0.39162306\n",
            "Iteration 100, loss = 0.39067330\n",
            "Iteration 101, loss = 0.38931125\n",
            "Iteration 102, loss = 0.38784867\n",
            "Iteration 103, loss = 0.38647444\n",
            "Iteration 104, loss = 0.38488643\n",
            "Iteration 105, loss = 0.38365249\n",
            "Iteration 106, loss = 0.38271579\n",
            "Iteration 107, loss = 0.38174732\n",
            "Iteration 108, loss = 0.38039816\n",
            "Iteration 109, loss = 0.37895280\n",
            "Iteration 110, loss = 0.37739644\n",
            "Iteration 111, loss = 0.37589848\n",
            "Iteration 112, loss = 0.37454104\n",
            "Iteration 113, loss = 0.37327269\n",
            "Iteration 114, loss = 0.37182456\n",
            "Iteration 115, loss = 0.37055837\n",
            "Iteration 116, loss = 0.36945851\n",
            "Iteration 117, loss = 0.36824700\n",
            "Iteration 118, loss = 0.36677075\n",
            "Iteration 119, loss = 0.36541532\n",
            "Iteration 120, loss = 0.36411716\n",
            "Iteration 121, loss = 0.36280556\n",
            "Iteration 122, loss = 0.36184741\n",
            "Iteration 123, loss = 0.36033588\n",
            "Iteration 124, loss = 0.35909444\n",
            "Iteration 125, loss = 0.35766109\n",
            "Iteration 126, loss = 0.35666216\n",
            "Iteration 127, loss = 0.35522733\n",
            "Iteration 128, loss = 0.35419154\n",
            "Iteration 129, loss = 0.35289610\n",
            "Iteration 130, loss = 0.35169062\n",
            "Iteration 131, loss = 0.35063556\n",
            "Iteration 132, loss = 0.34966123\n",
            "Iteration 133, loss = 0.34857702\n",
            "Iteration 134, loss = 0.34743588\n",
            "Iteration 135, loss = 0.34632621\n",
            "Iteration 136, loss = 0.34519245\n",
            "Iteration 137, loss = 0.34380917\n",
            "Iteration 138, loss = 0.34227218\n",
            "Iteration 139, loss = 0.34095878\n",
            "Iteration 140, loss = 0.33948183\n",
            "Iteration 141, loss = 0.33819012\n",
            "Iteration 142, loss = 0.33665632\n",
            "Iteration 143, loss = 0.33569960\n",
            "Iteration 144, loss = 0.33437375\n",
            "Iteration 145, loss = 0.33322321\n",
            "Iteration 146, loss = 0.33207726\n",
            "Iteration 147, loss = 0.33089849\n",
            "Iteration 148, loss = 0.33001390\n",
            "Iteration 149, loss = 0.32904644\n",
            "Iteration 150, loss = 0.32804475\n",
            "Iteration 151, loss = 0.32669803\n",
            "Iteration 152, loss = 0.32526058\n",
            "Iteration 153, loss = 0.32390644\n",
            "Iteration 154, loss = 0.32250597\n",
            "Iteration 155, loss = 0.32162343\n",
            "Iteration 156, loss = 0.32091202\n",
            "Iteration 157, loss = 0.31978472\n",
            "Iteration 158, loss = 0.31838106\n",
            "Iteration 159, loss = 0.31713661\n",
            "Iteration 160, loss = 0.31558437\n",
            "Iteration 161, loss = 0.31424745\n",
            "Iteration 162, loss = 0.31292619\n",
            "Iteration 163, loss = 0.31173893\n",
            "Iteration 164, loss = 0.31063782\n",
            "Iteration 165, loss = 0.30964063\n",
            "Iteration 166, loss = 0.30830167\n",
            "Iteration 167, loss = 0.30704766\n",
            "Iteration 168, loss = 0.30549599\n",
            "Iteration 169, loss = 0.30417018\n",
            "Iteration 170, loss = 0.30309655\n",
            "Iteration 171, loss = 0.30167642\n",
            "Iteration 172, loss = 0.30037906\n",
            "Iteration 173, loss = 0.29901573\n",
            "Iteration 174, loss = 0.29759745\n",
            "Iteration 175, loss = 0.29680958\n",
            "Iteration 176, loss = 0.29578081\n",
            "Iteration 177, loss = 0.29487615\n",
            "Iteration 178, loss = 0.29391185\n",
            "Iteration 179, loss = 0.29319064\n",
            "Iteration 180, loss = 0.29190226\n",
            "Iteration 181, loss = 0.29046201\n",
            "Iteration 182, loss = 0.28926097\n",
            "Iteration 183, loss = 0.28826380\n",
            "Iteration 184, loss = 0.28718516\n",
            "Iteration 185, loss = 0.28611990\n",
            "Iteration 186, loss = 0.28499796\n",
            "Iteration 187, loss = 0.28372349\n",
            "Iteration 188, loss = 0.28289331\n",
            "Iteration 189, loss = 0.28170884\n",
            "Iteration 190, loss = 0.28034841\n",
            "Iteration 191, loss = 0.27922650\n",
            "Iteration 192, loss = 0.27820068\n",
            "Iteration 193, loss = 0.27731652\n",
            "Iteration 194, loss = 0.27646285\n",
            "Iteration 195, loss = 0.27550596\n",
            "Iteration 196, loss = 0.27452537\n",
            "Iteration 197, loss = 0.27354364\n",
            "Iteration 198, loss = 0.27263913\n",
            "Iteration 199, loss = 0.27157911\n",
            "Iteration 200, loss = 0.27058968\n",
            "Iteration 201, loss = 0.26941491\n",
            "Iteration 202, loss = 0.26817209\n",
            "Iteration 203, loss = 0.26750453\n",
            "Iteration 204, loss = 0.26746049\n",
            "Iteration 205, loss = 0.26646544\n",
            "Iteration 206, loss = 0.26535399\n",
            "Iteration 207, loss = 0.26401622\n",
            "Iteration 208, loss = 0.26284004\n",
            "Iteration 209, loss = 0.26155655\n",
            "Iteration 210, loss = 0.26063882\n",
            "Iteration 211, loss = 0.25976254\n",
            "Iteration 212, loss = 0.25895820\n",
            "Iteration 213, loss = 0.25808448\n",
            "Iteration 214, loss = 0.25744813\n",
            "Iteration 215, loss = 0.25667301\n",
            "Iteration 216, loss = 0.25605832\n",
            "Iteration 217, loss = 0.25505597\n",
            "Iteration 218, loss = 0.25384519\n",
            "Iteration 219, loss = 0.25258576\n",
            "Iteration 220, loss = 0.25137510\n",
            "Iteration 221, loss = 0.25023288\n",
            "Iteration 222, loss = 0.24923263\n",
            "Iteration 223, loss = 0.24815835\n",
            "Iteration 224, loss = 0.24714575\n",
            "Iteration 225, loss = 0.24621982\n",
            "Iteration 226, loss = 0.24519726\n",
            "Iteration 227, loss = 0.24425276\n",
            "Iteration 228, loss = 0.24327651\n",
            "Iteration 229, loss = 0.24226524\n",
            "Iteration 230, loss = 0.24118581\n",
            "Iteration 231, loss = 0.24025651\n",
            "Iteration 232, loss = 0.23985096\n",
            "Iteration 233, loss = 0.24017626\n",
            "Iteration 234, loss = 0.23993245\n",
            "Iteration 235, loss = 0.23914638\n",
            "Iteration 236, loss = 0.23797865\n",
            "Iteration 237, loss = 0.23658034\n",
            "Iteration 238, loss = 0.23514183\n",
            "Iteration 239, loss = 0.23372449\n",
            "Iteration 240, loss = 0.23262574\n",
            "Iteration 241, loss = 0.23228424\n",
            "Iteration 242, loss = 0.23165407\n",
            "Iteration 243, loss = 0.23089105\n",
            "Iteration 244, loss = 0.23006043\n",
            "Iteration 245, loss = 0.22901491\n",
            "Iteration 246, loss = 0.22817379\n",
            "Iteration 247, loss = 0.22752613\n",
            "Iteration 248, loss = 0.22700729\n",
            "Iteration 249, loss = 0.22653001\n",
            "Iteration 250, loss = 0.22563937\n",
            "Iteration 251, loss = 0.22455317\n",
            "Iteration 252, loss = 0.22335180\n",
            "Iteration 253, loss = 0.22247480\n",
            "Iteration 254, loss = 0.22158197\n",
            "Iteration 255, loss = 0.22067488\n",
            "Iteration 256, loss = 0.21979754\n",
            "Iteration 257, loss = 0.21916193\n",
            "Iteration 258, loss = 0.21835588\n",
            "Iteration 259, loss = 0.21770104\n",
            "Iteration 260, loss = 0.21677495\n",
            "Iteration 261, loss = 0.21599701\n",
            "Iteration 262, loss = 0.21519940\n",
            "Iteration 263, loss = 0.21443131\n",
            "Iteration 264, loss = 0.21371627\n",
            "Iteration 265, loss = 0.21305328\n",
            "Iteration 266, loss = 0.21237561\n",
            "Iteration 267, loss = 0.21194720\n",
            "Iteration 268, loss = 0.21130591\n",
            "Iteration 269, loss = 0.21046621\n",
            "Iteration 270, loss = 0.20937999\n",
            "Iteration 271, loss = 0.20807679\n",
            "Iteration 272, loss = 0.20721823\n",
            "Iteration 273, loss = 0.20648952\n",
            "Iteration 274, loss = 0.20612377\n",
            "Iteration 275, loss = 0.20585092\n",
            "Iteration 276, loss = 0.20544233\n",
            "Iteration 277, loss = 0.20456726\n",
            "Iteration 278, loss = 0.20324482\n",
            "Iteration 279, loss = 0.20235740\n",
            "Iteration 280, loss = 0.20248586\n",
            "Iteration 281, loss = 0.20164741\n",
            "Iteration 282, loss = 0.20045628\n",
            "Iteration 283, loss = 0.19942275\n",
            "Iteration 284, loss = 0.19864538\n",
            "Iteration 285, loss = 0.19811446\n",
            "Iteration 286, loss = 0.19751834\n",
            "Iteration 287, loss = 0.19689801\n",
            "Iteration 288, loss = 0.19607505\n",
            "Iteration 289, loss = 0.19541519\n",
            "Iteration 290, loss = 0.19451673\n",
            "Iteration 291, loss = 0.19357777\n",
            "Iteration 292, loss = 0.19284617\n",
            "Iteration 293, loss = 0.19216184\n",
            "Iteration 294, loss = 0.19143295\n",
            "Iteration 295, loss = 0.19064991\n",
            "Iteration 296, loss = 0.18991056\n",
            "Iteration 297, loss = 0.18929598\n",
            "Iteration 298, loss = 0.18875505\n",
            "Iteration 299, loss = 0.18826002\n",
            "Iteration 300, loss = 0.18754971\n",
            "Iteration 301, loss = 0.18668051\n",
            "Iteration 302, loss = 0.18593561\n",
            "Iteration 303, loss = 0.18512448\n",
            "Iteration 304, loss = 0.18448056\n",
            "Iteration 305, loss = 0.18401498\n",
            "Iteration 306, loss = 0.18347511\n",
            "Iteration 307, loss = 0.18296770\n",
            "Iteration 308, loss = 0.18249168\n",
            "Iteration 309, loss = 0.18206344\n",
            "Iteration 310, loss = 0.18149730\n",
            "Iteration 311, loss = 0.18078266\n",
            "Iteration 312, loss = 0.18018719\n",
            "Iteration 313, loss = 0.17932733\n",
            "Iteration 314, loss = 0.17856354\n",
            "Iteration 315, loss = 0.17820089\n",
            "Iteration 316, loss = 0.17762488\n",
            "Iteration 317, loss = 0.17687789\n",
            "Iteration 318, loss = 0.17603877\n",
            "Iteration 319, loss = 0.17510315\n",
            "Iteration 320, loss = 0.17450601\n",
            "Iteration 321, loss = 0.17396689\n",
            "Iteration 322, loss = 0.17333183\n",
            "Iteration 323, loss = 0.17280409\n",
            "Iteration 324, loss = 0.17241946\n",
            "Iteration 325, loss = 0.17202015\n",
            "Iteration 326, loss = 0.17172286\n",
            "Iteration 327, loss = 0.17106178\n",
            "Iteration 328, loss = 0.17034526\n",
            "Iteration 329, loss = 0.16958293\n",
            "Iteration 330, loss = 0.16892674\n",
            "Iteration 331, loss = 0.16822838\n",
            "Iteration 332, loss = 0.16770449\n",
            "Iteration 333, loss = 0.16741441\n",
            "Iteration 334, loss = 0.16730384\n",
            "Iteration 335, loss = 0.16625555\n",
            "Iteration 336, loss = 0.16543342\n",
            "Iteration 337, loss = 0.16486183\n",
            "Iteration 338, loss = 0.16484991\n",
            "Iteration 339, loss = 0.16449703\n",
            "Iteration 340, loss = 0.16390617\n",
            "Iteration 341, loss = 0.16319685\n",
            "Iteration 342, loss = 0.16244662\n",
            "Iteration 343, loss = 0.16177073\n",
            "Iteration 344, loss = 0.16250193\n",
            "Iteration 345, loss = 0.16220037\n",
            "Iteration 346, loss = 0.16099937\n",
            "Iteration 347, loss = 0.15967338\n",
            "Iteration 348, loss = 0.15902919\n",
            "Iteration 349, loss = 0.15935125\n",
            "Iteration 350, loss = 0.15950287\n",
            "Iteration 351, loss = 0.15863406\n",
            "Iteration 352, loss = 0.15727892\n",
            "Iteration 353, loss = 0.15664576\n",
            "Iteration 354, loss = 0.15576038\n",
            "Iteration 355, loss = 0.15551436\n",
            "Iteration 356, loss = 0.15497394\n",
            "Iteration 357, loss = 0.15445017\n",
            "Iteration 358, loss = 0.15357278\n",
            "Iteration 359, loss = 0.15316313\n",
            "Iteration 360, loss = 0.15270586\n",
            "Iteration 361, loss = 0.15234575\n",
            "Iteration 362, loss = 0.15200557\n",
            "Iteration 363, loss = 0.15163866\n",
            "Iteration 364, loss = 0.15127578\n",
            "Iteration 365, loss = 0.15069140\n",
            "Iteration 366, loss = 0.15002968\n",
            "Iteration 367, loss = 0.14943257\n",
            "Iteration 368, loss = 0.14908677\n",
            "Iteration 369, loss = 0.14867205\n",
            "Iteration 370, loss = 0.14811306\n",
            "Iteration 371, loss = 0.14785324\n",
            "Iteration 372, loss = 0.14728690\n",
            "Iteration 373, loss = 0.14690020\n",
            "Iteration 374, loss = 0.14652153\n",
            "Iteration 375, loss = 0.14617706\n",
            "Iteration 376, loss = 0.14571586\n",
            "Iteration 377, loss = 0.14514959\n",
            "Iteration 378, loss = 0.14497702\n",
            "Iteration 379, loss = 0.14483788\n",
            "Iteration 380, loss = 0.14442415\n",
            "Iteration 381, loss = 0.14362190\n",
            "Iteration 382, loss = 0.14304459\n",
            "Iteration 383, loss = 0.14241708\n",
            "Iteration 384, loss = 0.14269311\n",
            "Iteration 385, loss = 0.14278073\n",
            "Iteration 386, loss = 0.14247604\n",
            "Iteration 387, loss = 0.14153128\n",
            "Iteration 388, loss = 0.14043027\n",
            "Iteration 389, loss = 0.13972916\n",
            "Iteration 390, loss = 0.13947105\n",
            "Iteration 391, loss = 0.13904728\n",
            "Iteration 392, loss = 0.13872926\n",
            "Iteration 393, loss = 0.13832576\n",
            "Iteration 394, loss = 0.13792702\n",
            "Iteration 395, loss = 0.13742319\n",
            "Iteration 396, loss = 0.13697609\n",
            "Iteration 397, loss = 0.13632907\n",
            "Iteration 398, loss = 0.13589088\n",
            "Iteration 399, loss = 0.13569909\n",
            "Iteration 400, loss = 0.13569421\n",
            "Iteration 401, loss = 0.13551266\n",
            "Iteration 402, loss = 0.13497208\n",
            "Iteration 403, loss = 0.13443653\n",
            "Iteration 404, loss = 0.13376965\n",
            "Iteration 405, loss = 0.13344804\n",
            "Iteration 406, loss = 0.13329896\n",
            "Iteration 407, loss = 0.13280927\n",
            "Iteration 408, loss = 0.13223471\n",
            "Iteration 409, loss = 0.13182439\n",
            "Iteration 410, loss = 0.13164557\n",
            "Iteration 411, loss = 0.13118405\n",
            "Iteration 412, loss = 0.13075964\n",
            "Iteration 413, loss = 0.13037486\n",
            "Iteration 414, loss = 0.12993583\n",
            "Iteration 415, loss = 0.12935280\n",
            "Iteration 416, loss = 0.12870809\n",
            "Iteration 417, loss = 0.12823353\n",
            "Iteration 418, loss = 0.12832338\n",
            "Iteration 419, loss = 0.12777211\n",
            "Iteration 420, loss = 0.12725069\n",
            "Iteration 421, loss = 0.12686612\n",
            "Iteration 422, loss = 0.12655919\n",
            "Iteration 423, loss = 0.12628948\n",
            "Iteration 424, loss = 0.12603270\n",
            "Iteration 425, loss = 0.12593868\n",
            "Iteration 426, loss = 0.12573682\n",
            "Iteration 427, loss = 0.12537117\n",
            "Iteration 428, loss = 0.12499160\n",
            "Iteration 429, loss = 0.12452803\n",
            "Iteration 430, loss = 0.12379690\n",
            "Iteration 431, loss = 0.12365661\n",
            "Iteration 432, loss = 0.12380170\n",
            "Iteration 433, loss = 0.12349731\n",
            "Iteration 434, loss = 0.12309994\n",
            "Iteration 435, loss = 0.12260741\n",
            "Iteration 436, loss = 0.12211874\n",
            "Iteration 437, loss = 0.12164742\n",
            "Iteration 438, loss = 0.12135512\n",
            "Iteration 439, loss = 0.12103616\n",
            "Iteration 440, loss = 0.12070662\n",
            "Iteration 441, loss = 0.12043941\n",
            "Iteration 442, loss = 0.11995565\n",
            "Iteration 443, loss = 0.11959294\n",
            "Iteration 444, loss = 0.11907689\n",
            "Iteration 445, loss = 0.11869015\n",
            "Iteration 446, loss = 0.11843171\n",
            "Iteration 447, loss = 0.11827647\n",
            "Iteration 448, loss = 0.11836311\n",
            "Iteration 449, loss = 0.11831756\n",
            "Iteration 450, loss = 0.11750496\n",
            "Iteration 451, loss = 0.11663424\n",
            "Iteration 452, loss = 0.11648656\n",
            "Iteration 453, loss = 0.11633469\n",
            "Iteration 454, loss = 0.11617314\n",
            "Iteration 455, loss = 0.11567531\n",
            "Iteration 456, loss = 0.11496983\n",
            "Iteration 457, loss = 0.11450089\n",
            "Iteration 458, loss = 0.11447283\n",
            "Iteration 459, loss = 0.11409431\n",
            "Iteration 460, loss = 0.11374317\n",
            "Iteration 461, loss = 0.11317015\n",
            "Iteration 462, loss = 0.11288927\n",
            "Iteration 463, loss = 0.11261251\n",
            "Iteration 464, loss = 0.11244196\n",
            "Iteration 465, loss = 0.11220539\n",
            "Iteration 466, loss = 0.11205831\n",
            "Iteration 467, loss = 0.11181896\n",
            "Iteration 468, loss = 0.11156880\n",
            "Iteration 469, loss = 0.11131691\n",
            "Iteration 470, loss = 0.11099281\n",
            "Iteration 471, loss = 0.11080683\n",
            "Iteration 472, loss = 0.11055529\n",
            "Iteration 473, loss = 0.11017952\n",
            "Iteration 474, loss = 0.10982055\n",
            "Iteration 475, loss = 0.10936611\n",
            "Iteration 476, loss = 0.10917635\n",
            "Iteration 477, loss = 0.10878656\n",
            "Iteration 478, loss = 0.10864706\n",
            "Iteration 479, loss = 0.10832441\n",
            "Iteration 480, loss = 0.10796642\n",
            "Iteration 481, loss = 0.10764648\n",
            "Iteration 482, loss = 0.10745969\n",
            "Iteration 483, loss = 0.10709211\n",
            "Iteration 484, loss = 0.10676537\n",
            "Iteration 485, loss = 0.10650840\n",
            "Iteration 486, loss = 0.10604898\n",
            "Iteration 487, loss = 0.10561993\n",
            "Iteration 488, loss = 0.10515372\n",
            "Iteration 489, loss = 0.10438060\n",
            "Iteration 490, loss = 0.10423186\n",
            "Iteration 491, loss = 0.10467888\n",
            "Iteration 492, loss = 0.10557959\n",
            "Iteration 493, loss = 0.10506383\n",
            "Iteration 494, loss = 0.10384358\n",
            "Iteration 495, loss = 0.10301173\n",
            "Iteration 496, loss = 0.10242589\n",
            "Iteration 497, loss = 0.10247779\n",
            "Iteration 498, loss = 0.10242395\n",
            "Iteration 499, loss = 0.10223896\n",
            "Iteration 500, loss = 0.10194849\n",
            "Iteration 501, loss = 0.10167047\n",
            "Iteration 502, loss = 0.10148142\n",
            "Iteration 503, loss = 0.10099133\n",
            "Iteration 504, loss = 0.10032920\n",
            "Iteration 505, loss = 0.09960574\n",
            "Iteration 506, loss = 0.09931378\n",
            "Iteration 507, loss = 0.09921598\n",
            "Iteration 508, loss = 0.09917923\n",
            "Iteration 509, loss = 0.09909431\n",
            "Iteration 510, loss = 0.09876955\n",
            "Iteration 511, loss = 0.09826811\n",
            "Iteration 512, loss = 0.09799845\n",
            "Iteration 513, loss = 0.09758334\n",
            "Iteration 514, loss = 0.09727031\n",
            "Iteration 515, loss = 0.09695490\n",
            "Iteration 516, loss = 0.09695510\n",
            "Iteration 517, loss = 0.09723114\n",
            "Iteration 518, loss = 0.09700089\n",
            "Iteration 519, loss = 0.09678499\n",
            "Iteration 520, loss = 0.09639831\n",
            "Iteration 521, loss = 0.09601190\n",
            "Iteration 522, loss = 0.09581142\n",
            "Iteration 523, loss = 0.09545854\n",
            "Iteration 524, loss = 0.09526685\n",
            "Iteration 525, loss = 0.09512076\n",
            "Iteration 526, loss = 0.09494941\n",
            "Iteration 527, loss = 0.09476062\n",
            "Iteration 528, loss = 0.09456184\n",
            "Iteration 529, loss = 0.09431400\n",
            "Iteration 530, loss = 0.09406400\n",
            "Iteration 531, loss = 0.09446089\n",
            "Iteration 532, loss = 0.09440711\n",
            "Iteration 533, loss = 0.09402242\n",
            "Iteration 534, loss = 0.09341819\n",
            "Iteration 535, loss = 0.09306484\n",
            "Iteration 536, loss = 0.09271265\n",
            "Iteration 537, loss = 0.09251144\n",
            "Iteration 538, loss = 0.09226833\n",
            "Iteration 539, loss = 0.09200153\n",
            "Iteration 540, loss = 0.09169607\n",
            "Iteration 541, loss = 0.09152643\n",
            "Iteration 542, loss = 0.09127783\n",
            "Iteration 543, loss = 0.09104690\n",
            "Iteration 544, loss = 0.09095247\n",
            "Iteration 545, loss = 0.09072949\n",
            "Iteration 546, loss = 0.09045434\n",
            "Iteration 547, loss = 0.09044308\n",
            "Iteration 548, loss = 0.09056990\n",
            "Iteration 549, loss = 0.09052608\n",
            "Iteration 550, loss = 0.09033886\n",
            "Iteration 551, loss = 0.08988700\n",
            "Iteration 552, loss = 0.08943493\n",
            "Iteration 553, loss = 0.08917300\n",
            "Iteration 554, loss = 0.08888575\n",
            "Iteration 555, loss = 0.08857660\n",
            "Iteration 556, loss = 0.08831927\n",
            "Iteration 557, loss = 0.08794008\n",
            "Iteration 558, loss = 0.08754754\n",
            "Iteration 559, loss = 0.08722677\n",
            "Iteration 560, loss = 0.08698318\n",
            "Iteration 561, loss = 0.08675095\n",
            "Iteration 562, loss = 0.08643023\n",
            "Iteration 563, loss = 0.08620680\n",
            "Iteration 564, loss = 0.08581514\n",
            "Iteration 565, loss = 0.08553985\n",
            "Iteration 566, loss = 0.08534332\n",
            "Iteration 567, loss = 0.08503947\n",
            "Iteration 568, loss = 0.08482841\n",
            "Iteration 569, loss = 0.08477253\n",
            "Iteration 570, loss = 0.08442148\n",
            "Iteration 571, loss = 0.08421861\n",
            "Iteration 572, loss = 0.08404310\n",
            "Iteration 573, loss = 0.08390492\n",
            "Iteration 574, loss = 0.08370198\n",
            "Iteration 575, loss = 0.08359538\n",
            "Iteration 576, loss = 0.08345764\n",
            "Iteration 577, loss = 0.08315839\n",
            "Iteration 578, loss = 0.08312282\n",
            "Iteration 579, loss = 0.08295278\n",
            "Iteration 580, loss = 0.08288903\n",
            "Iteration 581, loss = 0.08263005\n",
            "Iteration 582, loss = 0.08242728\n",
            "Iteration 583, loss = 0.08234840\n",
            "Iteration 584, loss = 0.08192091\n",
            "Iteration 585, loss = 0.08166045\n",
            "Iteration 586, loss = 0.08128627\n",
            "Iteration 587, loss = 0.08099438\n",
            "Iteration 588, loss = 0.08088598\n",
            "Iteration 589, loss = 0.08072314\n",
            "Iteration 590, loss = 0.08068380\n",
            "Iteration 591, loss = 0.08055446\n",
            "Iteration 592, loss = 0.08045782\n",
            "Iteration 593, loss = 0.08030625\n",
            "Iteration 594, loss = 0.08012997\n",
            "Iteration 595, loss = 0.07990657\n",
            "Iteration 596, loss = 0.07953668\n",
            "Iteration 597, loss = 0.07923936\n",
            "Iteration 598, loss = 0.07938227\n",
            "Iteration 599, loss = 0.07944736\n",
            "Iteration 600, loss = 0.07936966\n",
            "Iteration 601, loss = 0.07924163\n",
            "Iteration 602, loss = 0.07897444\n",
            "Iteration 603, loss = 0.07860871\n",
            "Iteration 604, loss = 0.07828293\n",
            "Iteration 605, loss = 0.07819638\n",
            "Iteration 606, loss = 0.07860530\n",
            "Iteration 607, loss = 0.07842698\n",
            "Iteration 608, loss = 0.07803598\n",
            "Iteration 609, loss = 0.07767194\n",
            "Iteration 610, loss = 0.07729421\n",
            "Iteration 611, loss = 0.07718835\n",
            "Iteration 612, loss = 0.07748916\n",
            "Iteration 613, loss = 0.07757403\n",
            "Iteration 614, loss = 0.07751093\n",
            "Iteration 615, loss = 0.07748623\n",
            "Iteration 616, loss = 0.07718430\n",
            "Iteration 617, loss = 0.07675890\n",
            "Iteration 618, loss = 0.07634719\n",
            "Iteration 619, loss = 0.07605242\n",
            "Iteration 620, loss = 0.07586891\n",
            "Iteration 621, loss = 0.07572204\n",
            "Iteration 622, loss = 0.07563836\n",
            "Iteration 623, loss = 0.07563340\n",
            "Iteration 624, loss = 0.07538676\n",
            "Iteration 625, loss = 0.07533880\n",
            "Iteration 626, loss = 0.07568122\n",
            "Iteration 627, loss = 0.07572521\n",
            "Iteration 628, loss = 0.07549983\n",
            "Iteration 629, loss = 0.07508516\n",
            "Iteration 630, loss = 0.07464270\n",
            "Iteration 631, loss = 0.07432457\n",
            "Iteration 632, loss = 0.07386104\n",
            "Iteration 633, loss = 0.07367962\n",
            "Iteration 634, loss = 0.07370893\n",
            "Iteration 635, loss = 0.07386296\n",
            "Iteration 636, loss = 0.07424859\n",
            "Iteration 637, loss = 0.07425928\n",
            "Iteration 638, loss = 0.07388299\n",
            "Iteration 639, loss = 0.07319454\n",
            "Iteration 640, loss = 0.07276199\n",
            "Iteration 641, loss = 0.07244711\n",
            "Iteration 642, loss = 0.07206624\n",
            "Iteration 643, loss = 0.07220759\n",
            "Iteration 644, loss = 0.07316690\n",
            "Iteration 645, loss = 0.07341968\n",
            "Iteration 646, loss = 0.07317858\n",
            "Iteration 647, loss = 0.07262086\n",
            "Iteration 648, loss = 0.07211154\n",
            "Iteration 649, loss = 0.07192317\n",
            "Iteration 650, loss = 0.07147597\n",
            "Iteration 651, loss = 0.07118131\n",
            "Iteration 652, loss = 0.07134206\n",
            "Iteration 653, loss = 0.07130663\n",
            "Iteration 654, loss = 0.07121539\n",
            "Iteration 655, loss = 0.07098460\n",
            "Iteration 656, loss = 0.07075733\n",
            "Iteration 657, loss = 0.07047507\n",
            "Iteration 658, loss = 0.07019037\n",
            "Iteration 659, loss = 0.07013079\n",
            "Iteration 660, loss = 0.07033899\n",
            "Iteration 661, loss = 0.07043163\n",
            "Iteration 662, loss = 0.07036312\n",
            "Iteration 663, loss = 0.07038907\n",
            "Iteration 664, loss = 0.07048990\n",
            "Iteration 665, loss = 0.07010363\n",
            "Iteration 666, loss = 0.06967943\n",
            "Iteration 667, loss = 0.06927726\n",
            "Iteration 668, loss = 0.06891410\n",
            "Iteration 669, loss = 0.06857874\n",
            "Iteration 670, loss = 0.06859011\n",
            "Iteration 671, loss = 0.06843388\n",
            "Iteration 672, loss = 0.06862000\n",
            "Iteration 673, loss = 0.06908024\n",
            "Iteration 674, loss = 0.06929281\n",
            "Iteration 675, loss = 0.06922195\n",
            "Iteration 676, loss = 0.06914594\n",
            "Iteration 677, loss = 0.06845813\n",
            "Iteration 678, loss = 0.06784168\n",
            "Iteration 679, loss = 0.06784303\n",
            "Iteration 680, loss = 0.06775669\n",
            "Iteration 681, loss = 0.06756367\n",
            "Iteration 682, loss = 0.06732521\n",
            "Iteration 683, loss = 0.06715084\n",
            "Iteration 684, loss = 0.06719932\n",
            "Iteration 685, loss = 0.06752299\n",
            "Iteration 686, loss = 0.06740411\n",
            "Iteration 687, loss = 0.06714772\n",
            "Iteration 688, loss = 0.06687380\n",
            "Iteration 689, loss = 0.06661802\n",
            "Iteration 690, loss = 0.06640112\n",
            "Iteration 691, loss = 0.06615629\n",
            "Iteration 692, loss = 0.06602497\n",
            "Iteration 693, loss = 0.06575910\n",
            "Iteration 694, loss = 0.06546320\n",
            "Iteration 695, loss = 0.06521883\n",
            "Iteration 696, loss = 0.06513702\n",
            "Iteration 697, loss = 0.06493147\n",
            "Iteration 698, loss = 0.06479348\n",
            "Iteration 699, loss = 0.06459310\n",
            "Iteration 700, loss = 0.06456365\n",
            "Iteration 701, loss = 0.06444332\n",
            "Iteration 702, loss = 0.06433476\n",
            "Iteration 703, loss = 0.06420259\n",
            "Iteration 704, loss = 0.06403779\n",
            "Iteration 705, loss = 0.06385706\n",
            "Iteration 706, loss = 0.06369643\n",
            "Iteration 707, loss = 0.06365560\n",
            "Iteration 708, loss = 0.06345162\n",
            "Iteration 709, loss = 0.06316328\n",
            "Iteration 710, loss = 0.06296842\n",
            "Iteration 711, loss = 0.06290213\n",
            "Iteration 712, loss = 0.06282244\n",
            "Iteration 713, loss = 0.06274324\n",
            "Iteration 714, loss = 0.06273340\n",
            "Iteration 715, loss = 0.06270782\n",
            "Iteration 716, loss = 0.06265226\n",
            "Iteration 717, loss = 0.06260781\n",
            "Iteration 718, loss = 0.06234379\n",
            "Iteration 719, loss = 0.06218013\n",
            "Iteration 720, loss = 0.06206707\n",
            "Iteration 721, loss = 0.06189486\n",
            "Iteration 722, loss = 0.06175385\n",
            "Iteration 723, loss = 0.06166725\n",
            "Iteration 724, loss = 0.06150892\n",
            "Iteration 725, loss = 0.06135096\n",
            "Iteration 726, loss = 0.06121072\n",
            "Iteration 727, loss = 0.06112707\n",
            "Iteration 728, loss = 0.06101283\n",
            "Iteration 729, loss = 0.06083778\n",
            "Iteration 730, loss = 0.06079887\n",
            "Iteration 731, loss = 0.06080247\n",
            "Iteration 732, loss = 0.06086983\n",
            "Iteration 733, loss = 0.06067476\n",
            "Iteration 734, loss = 0.06038383\n",
            "Iteration 735, loss = 0.06024671\n",
            "Iteration 736, loss = 0.06040273\n",
            "Iteration 737, loss = 0.06045516\n",
            "Iteration 738, loss = 0.06050425\n",
            "Iteration 739, loss = 0.06066386\n",
            "Iteration 740, loss = 0.06063539\n",
            "Iteration 741, loss = 0.06027729\n",
            "Iteration 742, loss = 0.05973052\n",
            "Iteration 743, loss = 0.05953069\n",
            "Iteration 744, loss = 0.05947436\n",
            "Iteration 745, loss = 0.05944064\n",
            "Iteration 746, loss = 0.05942835\n",
            "Iteration 747, loss = 0.05957505\n",
            "Iteration 748, loss = 0.05937444\n",
            "Iteration 749, loss = 0.05910641\n",
            "Iteration 750, loss = 0.05883590\n",
            "Iteration 751, loss = 0.05870121\n",
            "Iteration 752, loss = 0.05868696\n",
            "Iteration 753, loss = 0.05914221\n",
            "Iteration 754, loss = 0.05962244\n",
            "Iteration 755, loss = 0.05968890\n",
            "Iteration 756, loss = 0.05912590\n",
            "Iteration 757, loss = 0.05850735\n",
            "Iteration 758, loss = 0.05818181\n",
            "Iteration 759, loss = 0.05800654\n",
            "Iteration 760, loss = 0.05789061\n",
            "Iteration 761, loss = 0.05783759\n",
            "Iteration 762, loss = 0.05772457\n",
            "Iteration 763, loss = 0.05761512\n",
            "Iteration 764, loss = 0.05752388\n",
            "Iteration 765, loss = 0.05748123\n",
            "Iteration 766, loss = 0.05729056\n",
            "Iteration 767, loss = 0.05714037\n",
            "Iteration 768, loss = 0.05702553\n",
            "Iteration 769, loss = 0.05684705\n",
            "Iteration 770, loss = 0.05686850\n",
            "Iteration 771, loss = 0.05681080\n",
            "Iteration 772, loss = 0.05670788\n",
            "Iteration 773, loss = 0.05663932\n",
            "Iteration 774, loss = 0.05655610\n",
            "Iteration 775, loss = 0.05649025\n",
            "Iteration 776, loss = 0.05643414\n",
            "Iteration 777, loss = 0.05635294\n",
            "Iteration 778, loss = 0.05633003\n",
            "Iteration 779, loss = 0.05633929\n",
            "Iteration 780, loss = 0.05623857\n",
            "Iteration 781, loss = 0.05592996\n",
            "Iteration 782, loss = 0.05586906\n",
            "Iteration 783, loss = 0.05572606\n",
            "Iteration 784, loss = 0.05562101\n",
            "Iteration 785, loss = 0.05550792\n",
            "Iteration 786, loss = 0.05567816\n",
            "Iteration 787, loss = 0.05551520\n",
            "Iteration 788, loss = 0.05551084\n",
            "Iteration 789, loss = 0.05549447\n",
            "Iteration 790, loss = 0.05561588\n",
            "Iteration 791, loss = 0.05574600\n",
            "Iteration 792, loss = 0.05587251\n",
            "Iteration 793, loss = 0.05584817\n",
            "Iteration 794, loss = 0.05570360\n",
            "Iteration 795, loss = 0.05551377\n",
            "Iteration 796, loss = 0.05518514\n",
            "Iteration 797, loss = 0.05492356\n",
            "Iteration 798, loss = 0.05465856\n",
            "Iteration 799, loss = 0.05456231\n",
            "Iteration 800, loss = 0.05460907\n",
            "Iteration 801, loss = 0.05471039\n",
            "Iteration 802, loss = 0.05461380\n",
            "Iteration 803, loss = 0.05445833\n",
            "Iteration 804, loss = 0.05443341\n",
            "Iteration 805, loss = 0.05428780\n",
            "Iteration 806, loss = 0.05427432\n",
            "Iteration 807, loss = 0.05405058\n",
            "Iteration 808, loss = 0.05384712\n",
            "Iteration 809, loss = 0.05367847\n",
            "Iteration 810, loss = 0.05358398\n",
            "Iteration 811, loss = 0.05347084\n",
            "Iteration 812, loss = 0.05355412\n",
            "Iteration 813, loss = 0.05354642\n",
            "Iteration 814, loss = 0.05379276\n",
            "Iteration 815, loss = 0.05400052\n",
            "Iteration 816, loss = 0.05379189\n",
            "Iteration 817, loss = 0.05337321\n",
            "Iteration 818, loss = 0.05327152\n",
            "Iteration 819, loss = 0.05285515\n",
            "Iteration 820, loss = 0.05266727\n",
            "Iteration 821, loss = 0.05254867\n",
            "Iteration 822, loss = 0.05239772\n",
            "Iteration 823, loss = 0.05235634\n",
            "Iteration 824, loss = 0.05234947\n",
            "Iteration 825, loss = 0.05260810\n",
            "Iteration 826, loss = 0.05252531\n",
            "Iteration 827, loss = 0.05242204\n",
            "Iteration 828, loss = 0.05239894\n",
            "Iteration 829, loss = 0.05238753\n",
            "Iteration 830, loss = 0.05262689\n",
            "Iteration 831, loss = 0.05229629\n",
            "Iteration 832, loss = 0.05209195\n",
            "Iteration 833, loss = 0.05210415\n",
            "Iteration 834, loss = 0.05227522\n",
            "Iteration 835, loss = 0.05206476\n",
            "Iteration 836, loss = 0.05181329\n",
            "Iteration 837, loss = 0.05178830\n",
            "Iteration 838, loss = 0.05154752\n",
            "Iteration 839, loss = 0.05144679\n",
            "Iteration 840, loss = 0.05132264\n",
            "Iteration 841, loss = 0.05144860\n",
            "Iteration 842, loss = 0.05121572\n",
            "Iteration 843, loss = 0.05103368\n",
            "Iteration 844, loss = 0.05109747\n",
            "Iteration 845, loss = 0.05140017\n",
            "Iteration 846, loss = 0.05141727\n",
            "Iteration 847, loss = 0.05114792\n",
            "Iteration 848, loss = 0.05095768\n",
            "Iteration 849, loss = 0.05055282\n",
            "Iteration 850, loss = 0.05051086\n",
            "Iteration 851, loss = 0.05060932\n",
            "Iteration 852, loss = 0.05037048\n",
            "Iteration 853, loss = 0.05029093\n",
            "Iteration 854, loss = 0.05027227\n",
            "Iteration 855, loss = 0.05012807\n",
            "Iteration 856, loss = 0.05000915\n",
            "Iteration 857, loss = 0.04998202\n",
            "Iteration 858, loss = 0.04981984\n",
            "Iteration 859, loss = 0.04968881\n",
            "Iteration 860, loss = 0.04966013\n",
            "Iteration 861, loss = 0.04960904\n",
            "Iteration 862, loss = 0.04938767\n",
            "Iteration 863, loss = 0.04937489\n",
            "Iteration 864, loss = 0.04948377\n",
            "Iteration 865, loss = 0.04991894\n",
            "Iteration 866, loss = 0.05003203\n",
            "Iteration 867, loss = 0.05003079\n",
            "Iteration 868, loss = 0.04995493\n",
            "Iteration 869, loss = 0.04983707\n",
            "Iteration 870, loss = 0.04973869\n",
            "Iteration 871, loss = 0.04945417\n",
            "Iteration 872, loss = 0.04940746\n",
            "Iteration 873, loss = 0.04931023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faça outras alterações nos parâmetros**\n",
        "\n",
        "**4 entradas - 3 neurônios - 3 neurônios - 1**\n",
        "\n",
        "**Veja SoftMax para problemas multiclasse**"
      ],
      "metadata": {
        "id": "7PQZVMhEJXOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural = MLPClassifier(max_iter=1000, verbose=True, tol=0.00000000000001, solver = 'adam', activation = 'relu', hidden_layer_sizes = 9)\n",
        "modelo.fit(X_treino, y_treino)"
      ],
      "metadata": {
        "id": "C6Q1RssrJU9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802b6904-3a64-4a71-fd3b-221c672f3524"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.72306035\n",
            "Iteration 2, loss = 0.69633308\n",
            "Iteration 3, loss = 0.67320514\n",
            "Iteration 4, loss = 0.65343187\n",
            "Iteration 5, loss = 0.63608752\n",
            "Iteration 6, loss = 0.62157027\n",
            "Iteration 7, loss = 0.60919457\n",
            "Iteration 8, loss = 0.59918153\n",
            "Iteration 9, loss = 0.58984938\n",
            "Iteration 10, loss = 0.58222365\n",
            "Iteration 11, loss = 0.57624203\n",
            "Iteration 12, loss = 0.57043246\n",
            "Iteration 13, loss = 0.56483057\n",
            "Iteration 14, loss = 0.56047715\n",
            "Iteration 15, loss = 0.55570116\n",
            "Iteration 16, loss = 0.55150399\n",
            "Iteration 17, loss = 0.54718731\n",
            "Iteration 18, loss = 0.54324718\n",
            "Iteration 19, loss = 0.53924800\n",
            "Iteration 20, loss = 0.53531778\n",
            "Iteration 21, loss = 0.53172883\n",
            "Iteration 22, loss = 0.52814964\n",
            "Iteration 23, loss = 0.52485392\n",
            "Iteration 24, loss = 0.52152478\n",
            "Iteration 25, loss = 0.51828858\n",
            "Iteration 26, loss = 0.51524400\n",
            "Iteration 27, loss = 0.51197622\n",
            "Iteration 28, loss = 0.50893082\n",
            "Iteration 29, loss = 0.50604187\n",
            "Iteration 30, loss = 0.50335033\n",
            "Iteration 31, loss = 0.50056205\n",
            "Iteration 32, loss = 0.49829514\n",
            "Iteration 33, loss = 0.49588888\n",
            "Iteration 34, loss = 0.49348036\n",
            "Iteration 35, loss = 0.49101827\n",
            "Iteration 36, loss = 0.48857963\n",
            "Iteration 37, loss = 0.48624603\n",
            "Iteration 38, loss = 0.48376188\n",
            "Iteration 39, loss = 0.48106891\n",
            "Iteration 40, loss = 0.47847430\n",
            "Iteration 41, loss = 0.47586977\n",
            "Iteration 42, loss = 0.47349219\n",
            "Iteration 43, loss = 0.47154989\n",
            "Iteration 44, loss = 0.46937815\n",
            "Iteration 45, loss = 0.46732053\n",
            "Iteration 46, loss = 0.46534331\n",
            "Iteration 47, loss = 0.46350366\n",
            "Iteration 48, loss = 0.46144615\n",
            "Iteration 49, loss = 0.45944706\n",
            "Iteration 50, loss = 0.45792240\n",
            "Iteration 51, loss = 0.45603998\n",
            "Iteration 52, loss = 0.45434814\n",
            "Iteration 53, loss = 0.45265301\n",
            "Iteration 54, loss = 0.45090082\n",
            "Iteration 55, loss = 0.44922907\n",
            "Iteration 56, loss = 0.44777613\n",
            "Iteration 57, loss = 0.44606487\n",
            "Iteration 58, loss = 0.44452422\n",
            "Iteration 59, loss = 0.44307456\n",
            "Iteration 60, loss = 0.44164743\n",
            "Iteration 61, loss = 0.44022808\n",
            "Iteration 62, loss = 0.43864593\n",
            "Iteration 63, loss = 0.43698198\n",
            "Iteration 64, loss = 0.43549209\n",
            "Iteration 65, loss = 0.43400155\n",
            "Iteration 66, loss = 0.43240155\n",
            "Iteration 67, loss = 0.43087825\n",
            "Iteration 68, loss = 0.42928122\n",
            "Iteration 69, loss = 0.42776678\n",
            "Iteration 70, loss = 0.42635682\n",
            "Iteration 71, loss = 0.42505359\n",
            "Iteration 72, loss = 0.42349959\n",
            "Iteration 73, loss = 0.42202989\n",
            "Iteration 74, loss = 0.42071459\n",
            "Iteration 75, loss = 0.41922027\n",
            "Iteration 76, loss = 0.41775358\n",
            "Iteration 77, loss = 0.41652875\n",
            "Iteration 78, loss = 0.41512882\n",
            "Iteration 79, loss = 0.41386907\n",
            "Iteration 80, loss = 0.41224781\n",
            "Iteration 81, loss = 0.41082969\n",
            "Iteration 82, loss = 0.40935469\n",
            "Iteration 83, loss = 0.40775415\n",
            "Iteration 84, loss = 0.40611159\n",
            "Iteration 85, loss = 0.40465835\n",
            "Iteration 86, loss = 0.40353939\n",
            "Iteration 87, loss = 0.40231442\n",
            "Iteration 88, loss = 0.40086839\n",
            "Iteration 89, loss = 0.39930299\n",
            "Iteration 90, loss = 0.39778984\n",
            "Iteration 91, loss = 0.39633180\n",
            "Iteration 92, loss = 0.39473205\n",
            "Iteration 93, loss = 0.39332329\n",
            "Iteration 94, loss = 0.39197797\n",
            "Iteration 95, loss = 0.39060623\n",
            "Iteration 96, loss = 0.38914970\n",
            "Iteration 97, loss = 0.38769257\n",
            "Iteration 98, loss = 0.38622746\n",
            "Iteration 99, loss = 0.38464386\n",
            "Iteration 100, loss = 0.38351029\n",
            "Iteration 101, loss = 0.38223333\n",
            "Iteration 102, loss = 0.38117908\n",
            "Iteration 103, loss = 0.38017264\n",
            "Iteration 104, loss = 0.37838484\n",
            "Iteration 105, loss = 0.37661316\n",
            "Iteration 106, loss = 0.37501066\n",
            "Iteration 107, loss = 0.37390568\n",
            "Iteration 108, loss = 0.37231343\n",
            "Iteration 109, loss = 0.37084845\n",
            "Iteration 110, loss = 0.36930879\n",
            "Iteration 111, loss = 0.36774441\n",
            "Iteration 112, loss = 0.36637167\n",
            "Iteration 113, loss = 0.36524841\n",
            "Iteration 114, loss = 0.36416797\n",
            "Iteration 115, loss = 0.36352525\n",
            "Iteration 116, loss = 0.36240069\n",
            "Iteration 117, loss = 0.36078396\n",
            "Iteration 118, loss = 0.35917152\n",
            "Iteration 119, loss = 0.35753453\n",
            "Iteration 120, loss = 0.35600500\n",
            "Iteration 121, loss = 0.35428524\n",
            "Iteration 122, loss = 0.35268164\n",
            "Iteration 123, loss = 0.35109078\n",
            "Iteration 124, loss = 0.34960645\n",
            "Iteration 125, loss = 0.34834941\n",
            "Iteration 126, loss = 0.34686832\n",
            "Iteration 127, loss = 0.34552436\n",
            "Iteration 128, loss = 0.34428952\n",
            "Iteration 129, loss = 0.34304752\n",
            "Iteration 130, loss = 0.34169101\n",
            "Iteration 131, loss = 0.34029158\n",
            "Iteration 132, loss = 0.33900742\n",
            "Iteration 133, loss = 0.33781937\n",
            "Iteration 134, loss = 0.33662568\n",
            "Iteration 135, loss = 0.33526648\n",
            "Iteration 136, loss = 0.33416667\n",
            "Iteration 137, loss = 0.33295943\n",
            "Iteration 138, loss = 0.33169658\n",
            "Iteration 139, loss = 0.33026739\n",
            "Iteration 140, loss = 0.32893757\n",
            "Iteration 141, loss = 0.32769266\n",
            "Iteration 142, loss = 0.32623374\n",
            "Iteration 143, loss = 0.32492536\n",
            "Iteration 144, loss = 0.32354807\n",
            "Iteration 145, loss = 0.32225383\n",
            "Iteration 146, loss = 0.32092440\n",
            "Iteration 147, loss = 0.31953908\n",
            "Iteration 148, loss = 0.31829409\n",
            "Iteration 149, loss = 0.31692842\n",
            "Iteration 150, loss = 0.31639302\n",
            "Iteration 151, loss = 0.31478920\n",
            "Iteration 152, loss = 0.31317505\n",
            "Iteration 153, loss = 0.31142711\n",
            "Iteration 154, loss = 0.30990118\n",
            "Iteration 155, loss = 0.30886387\n",
            "Iteration 156, loss = 0.30783565\n",
            "Iteration 157, loss = 0.30684641\n",
            "Iteration 158, loss = 0.30572590\n",
            "Iteration 159, loss = 0.30428905\n",
            "Iteration 160, loss = 0.30278351\n",
            "Iteration 161, loss = 0.30110964\n",
            "Iteration 162, loss = 0.29950707\n",
            "Iteration 163, loss = 0.29818382\n",
            "Iteration 164, loss = 0.29697570\n",
            "Iteration 165, loss = 0.29575100\n",
            "Iteration 166, loss = 0.29445169\n",
            "Iteration 167, loss = 0.29321168\n",
            "Iteration 168, loss = 0.29201269\n",
            "Iteration 169, loss = 0.29050594\n",
            "Iteration 170, loss = 0.28919206\n",
            "Iteration 171, loss = 0.28786486\n",
            "Iteration 172, loss = 0.28688682\n",
            "Iteration 173, loss = 0.28574270\n",
            "Iteration 174, loss = 0.28446725\n",
            "Iteration 175, loss = 0.28321651\n",
            "Iteration 176, loss = 0.28216078\n",
            "Iteration 177, loss = 0.28095062\n",
            "Iteration 178, loss = 0.27939966\n",
            "Iteration 179, loss = 0.27832308\n",
            "Iteration 180, loss = 0.27702349\n",
            "Iteration 181, loss = 0.27605231\n",
            "Iteration 182, loss = 0.27534741\n",
            "Iteration 183, loss = 0.27439521\n",
            "Iteration 184, loss = 0.27358255\n",
            "Iteration 185, loss = 0.27259614\n",
            "Iteration 186, loss = 0.27181551\n",
            "Iteration 187, loss = 0.27085102\n",
            "Iteration 188, loss = 0.26950385\n",
            "Iteration 189, loss = 0.26814267\n",
            "Iteration 190, loss = 0.26648412\n",
            "Iteration 191, loss = 0.26481779\n",
            "Iteration 192, loss = 0.26477722\n",
            "Iteration 193, loss = 0.26423883\n",
            "Iteration 194, loss = 0.26335843\n",
            "Iteration 195, loss = 0.26197912\n",
            "Iteration 196, loss = 0.26063552\n",
            "Iteration 197, loss = 0.25908899\n",
            "Iteration 198, loss = 0.25782271\n",
            "Iteration 199, loss = 0.25667254\n",
            "Iteration 200, loss = 0.25549232\n",
            "Iteration 201, loss = 0.25438703\n",
            "Iteration 202, loss = 0.25333116\n",
            "Iteration 203, loss = 0.25221451\n",
            "Iteration 204, loss = 0.25119086\n",
            "Iteration 205, loss = 0.25005236\n",
            "Iteration 206, loss = 0.24878739\n",
            "Iteration 207, loss = 0.24755890\n",
            "Iteration 208, loss = 0.24640053\n",
            "Iteration 209, loss = 0.24541269\n",
            "Iteration 210, loss = 0.24440067\n",
            "Iteration 211, loss = 0.24338512\n",
            "Iteration 212, loss = 0.24227527\n",
            "Iteration 213, loss = 0.24110671\n",
            "Iteration 214, loss = 0.24044229\n",
            "Iteration 215, loss = 0.24011458\n",
            "Iteration 216, loss = 0.23959490\n",
            "Iteration 217, loss = 0.23892040\n",
            "Iteration 218, loss = 0.23823594\n",
            "Iteration 219, loss = 0.23724096\n",
            "Iteration 220, loss = 0.23604891\n",
            "Iteration 221, loss = 0.23504411\n",
            "Iteration 222, loss = 0.23416202\n",
            "Iteration 223, loss = 0.23316072\n",
            "Iteration 224, loss = 0.23239321\n",
            "Iteration 225, loss = 0.23142583\n",
            "Iteration 226, loss = 0.23020865\n",
            "Iteration 227, loss = 0.22940873\n",
            "Iteration 228, loss = 0.22897231\n",
            "Iteration 229, loss = 0.22953148\n",
            "Iteration 230, loss = 0.22864851\n",
            "Iteration 231, loss = 0.22684197\n",
            "Iteration 232, loss = 0.22458960\n",
            "Iteration 233, loss = 0.22307218\n",
            "Iteration 234, loss = 0.22270068\n",
            "Iteration 235, loss = 0.22266335\n",
            "Iteration 236, loss = 0.22238120\n",
            "Iteration 237, loss = 0.22181444\n",
            "Iteration 238, loss = 0.21967359\n",
            "Iteration 239, loss = 0.21824850\n",
            "Iteration 240, loss = 0.21772723\n",
            "Iteration 241, loss = 0.21692938\n",
            "Iteration 242, loss = 0.21585922\n",
            "Iteration 243, loss = 0.21464406\n",
            "Iteration 244, loss = 0.21361679\n",
            "Iteration 245, loss = 0.21273443\n",
            "Iteration 246, loss = 0.21181645\n",
            "Iteration 247, loss = 0.21084135\n",
            "Iteration 248, loss = 0.20995920\n",
            "Iteration 249, loss = 0.20926255\n",
            "Iteration 250, loss = 0.20876684\n",
            "Iteration 251, loss = 0.20807065\n",
            "Iteration 252, loss = 0.20720668\n",
            "Iteration 253, loss = 0.20620037\n",
            "Iteration 254, loss = 0.20498611\n",
            "Iteration 255, loss = 0.20393995\n",
            "Iteration 256, loss = 0.20296673\n",
            "Iteration 257, loss = 0.20228668\n",
            "Iteration 258, loss = 0.20158296\n",
            "Iteration 259, loss = 0.20114813\n",
            "Iteration 260, loss = 0.20048518\n",
            "Iteration 261, loss = 0.19930850\n",
            "Iteration 262, loss = 0.19823179\n",
            "Iteration 263, loss = 0.19740320\n",
            "Iteration 264, loss = 0.19674026\n",
            "Iteration 265, loss = 0.19633543\n",
            "Iteration 266, loss = 0.19545535\n",
            "Iteration 267, loss = 0.19426558\n",
            "Iteration 268, loss = 0.19314965\n",
            "Iteration 269, loss = 0.19270285\n",
            "Iteration 270, loss = 0.19217476\n",
            "Iteration 271, loss = 0.19148533\n",
            "Iteration 272, loss = 0.19052414\n",
            "Iteration 273, loss = 0.18988658\n",
            "Iteration 274, loss = 0.18877595\n",
            "Iteration 275, loss = 0.18815791\n",
            "Iteration 276, loss = 0.18737493\n",
            "Iteration 277, loss = 0.18662854\n",
            "Iteration 278, loss = 0.18594219\n",
            "Iteration 279, loss = 0.18508061\n",
            "Iteration 280, loss = 0.18437022\n",
            "Iteration 281, loss = 0.18367582\n",
            "Iteration 282, loss = 0.18333830\n",
            "Iteration 283, loss = 0.18331068\n",
            "Iteration 284, loss = 0.18289216\n",
            "Iteration 285, loss = 0.18203084\n",
            "Iteration 286, loss = 0.18106919\n",
            "Iteration 287, loss = 0.18012626\n",
            "Iteration 288, loss = 0.17931195\n",
            "Iteration 289, loss = 0.17864410\n",
            "Iteration 290, loss = 0.17784003\n",
            "Iteration 291, loss = 0.17703905\n",
            "Iteration 292, loss = 0.17628491\n",
            "Iteration 293, loss = 0.17561733\n",
            "Iteration 294, loss = 0.17481263\n",
            "Iteration 295, loss = 0.17458939\n",
            "Iteration 296, loss = 0.17415103\n",
            "Iteration 297, loss = 0.17384954\n",
            "Iteration 298, loss = 0.17358846\n",
            "Iteration 299, loss = 0.17319056\n",
            "Iteration 300, loss = 0.17262030\n",
            "Iteration 301, loss = 0.17160828\n",
            "Iteration 302, loss = 0.17087199\n",
            "Iteration 303, loss = 0.17010341\n",
            "Iteration 304, loss = 0.16932544\n",
            "Iteration 305, loss = 0.16860318\n",
            "Iteration 306, loss = 0.16796145\n",
            "Iteration 307, loss = 0.16739562\n",
            "Iteration 308, loss = 0.16683922\n",
            "Iteration 309, loss = 0.16629758\n",
            "Iteration 310, loss = 0.16584007\n",
            "Iteration 311, loss = 0.16536299\n",
            "Iteration 312, loss = 0.16491374\n",
            "Iteration 313, loss = 0.16435527\n",
            "Iteration 314, loss = 0.16385836\n",
            "Iteration 315, loss = 0.16320704\n",
            "Iteration 316, loss = 0.16242045\n",
            "Iteration 317, loss = 0.16175651\n",
            "Iteration 318, loss = 0.16122612\n",
            "Iteration 319, loss = 0.16065888\n",
            "Iteration 320, loss = 0.15996786\n",
            "Iteration 321, loss = 0.15935412\n",
            "Iteration 322, loss = 0.15875912\n",
            "Iteration 323, loss = 0.15827814\n",
            "Iteration 324, loss = 0.15779758\n",
            "Iteration 325, loss = 0.15725360\n",
            "Iteration 326, loss = 0.15652553\n",
            "Iteration 327, loss = 0.15619311\n",
            "Iteration 328, loss = 0.15555348\n",
            "Iteration 329, loss = 0.15499456\n",
            "Iteration 330, loss = 0.15425547\n",
            "Iteration 331, loss = 0.15399473\n",
            "Iteration 332, loss = 0.15348952\n",
            "Iteration 333, loss = 0.15317704\n",
            "Iteration 334, loss = 0.15262979\n",
            "Iteration 335, loss = 0.15183629\n",
            "Iteration 336, loss = 0.15122059\n",
            "Iteration 337, loss = 0.15072781\n",
            "Iteration 338, loss = 0.15010786\n",
            "Iteration 339, loss = 0.14950474\n",
            "Iteration 340, loss = 0.14889894\n",
            "Iteration 341, loss = 0.14847357\n",
            "Iteration 342, loss = 0.14818872\n",
            "Iteration 343, loss = 0.14848128\n",
            "Iteration 344, loss = 0.14804315\n",
            "Iteration 345, loss = 0.14728983\n",
            "Iteration 346, loss = 0.14641786\n",
            "Iteration 347, loss = 0.14576705\n",
            "Iteration 348, loss = 0.14516118\n",
            "Iteration 349, loss = 0.14456085\n",
            "Iteration 350, loss = 0.14396673\n",
            "Iteration 351, loss = 0.14342119\n",
            "Iteration 352, loss = 0.14301789\n",
            "Iteration 353, loss = 0.14268256\n",
            "Iteration 354, loss = 0.14232550\n",
            "Iteration 355, loss = 0.14179035\n",
            "Iteration 356, loss = 0.14137558\n",
            "Iteration 357, loss = 0.14069264\n",
            "Iteration 358, loss = 0.14030903\n",
            "Iteration 359, loss = 0.14008971\n",
            "Iteration 360, loss = 0.13962733\n",
            "Iteration 361, loss = 0.13906273\n",
            "Iteration 362, loss = 0.13879325\n",
            "Iteration 363, loss = 0.13838187\n",
            "Iteration 364, loss = 0.13819419\n",
            "Iteration 365, loss = 0.13744834\n",
            "Iteration 366, loss = 0.13672849\n",
            "Iteration 367, loss = 0.13599472\n",
            "Iteration 368, loss = 0.13601610\n",
            "Iteration 369, loss = 0.13663500\n",
            "Iteration 370, loss = 0.13641280\n",
            "Iteration 371, loss = 0.13572538\n",
            "Iteration 372, loss = 0.13484876\n",
            "Iteration 373, loss = 0.13374138\n",
            "Iteration 374, loss = 0.13317069\n",
            "Iteration 375, loss = 0.13275411\n",
            "Iteration 376, loss = 0.13227577\n",
            "Iteration 377, loss = 0.13216939\n",
            "Iteration 378, loss = 0.13189064\n",
            "Iteration 379, loss = 0.13139431\n",
            "Iteration 380, loss = 0.13097066\n",
            "Iteration 381, loss = 0.13068092\n",
            "Iteration 382, loss = 0.13034095\n",
            "Iteration 383, loss = 0.13000227\n",
            "Iteration 384, loss = 0.12956733\n",
            "Iteration 385, loss = 0.12907688\n",
            "Iteration 386, loss = 0.12874554\n",
            "Iteration 387, loss = 0.12824784\n",
            "Iteration 388, loss = 0.12764701\n",
            "Iteration 389, loss = 0.12695833\n",
            "Iteration 390, loss = 0.12632609\n",
            "Iteration 391, loss = 0.12573598\n",
            "Iteration 392, loss = 0.12543302\n",
            "Iteration 393, loss = 0.12516623\n",
            "Iteration 394, loss = 0.12476758\n",
            "Iteration 395, loss = 0.12496124\n",
            "Iteration 396, loss = 0.12491387\n",
            "Iteration 397, loss = 0.12424094\n",
            "Iteration 398, loss = 0.12328463\n",
            "Iteration 399, loss = 0.12287457\n",
            "Iteration 400, loss = 0.12228545\n",
            "Iteration 401, loss = 0.12187945\n",
            "Iteration 402, loss = 0.12150204\n",
            "Iteration 403, loss = 0.12112098\n",
            "Iteration 404, loss = 0.12074138\n",
            "Iteration 405, loss = 0.12008365\n",
            "Iteration 406, loss = 0.11959941\n",
            "Iteration 407, loss = 0.11919314\n",
            "Iteration 408, loss = 0.11877562\n",
            "Iteration 409, loss = 0.11853486\n",
            "Iteration 410, loss = 0.11852158\n",
            "Iteration 411, loss = 0.11856100\n",
            "Iteration 412, loss = 0.11845389\n",
            "Iteration 413, loss = 0.11808139\n",
            "Iteration 414, loss = 0.11760917\n",
            "Iteration 415, loss = 0.11677723\n",
            "Iteration 416, loss = 0.11626412\n",
            "Iteration 417, loss = 0.11612178\n",
            "Iteration 418, loss = 0.11635087\n",
            "Iteration 419, loss = 0.11761419\n",
            "Iteration 420, loss = 0.11881817\n",
            "Iteration 421, loss = 0.11832412\n",
            "Iteration 422, loss = 0.11660733\n",
            "Iteration 423, loss = 0.11511721\n",
            "Iteration 424, loss = 0.11404431\n",
            "Iteration 425, loss = 0.11352866\n",
            "Iteration 426, loss = 0.11351614\n",
            "Iteration 427, loss = 0.11309411\n",
            "Iteration 428, loss = 0.11252450\n",
            "Iteration 429, loss = 0.11179941\n",
            "Iteration 430, loss = 0.11137896\n",
            "Iteration 431, loss = 0.11111759\n",
            "Iteration 432, loss = 0.11091421\n",
            "Iteration 433, loss = 0.11066908\n",
            "Iteration 434, loss = 0.11036834\n",
            "Iteration 435, loss = 0.10972754\n",
            "Iteration 436, loss = 0.10912526\n",
            "Iteration 437, loss = 0.10871090\n",
            "Iteration 438, loss = 0.10842988\n",
            "Iteration 439, loss = 0.10832456\n",
            "Iteration 440, loss = 0.10790203\n",
            "Iteration 441, loss = 0.10757285\n",
            "Iteration 442, loss = 0.10697197\n",
            "Iteration 443, loss = 0.10655309\n",
            "Iteration 444, loss = 0.10625343\n",
            "Iteration 445, loss = 0.10587794\n",
            "Iteration 446, loss = 0.10559125\n",
            "Iteration 447, loss = 0.10532635\n",
            "Iteration 448, loss = 0.10494844\n",
            "Iteration 449, loss = 0.10454836\n",
            "Iteration 450, loss = 0.10433784\n",
            "Iteration 451, loss = 0.10431124\n",
            "Iteration 452, loss = 0.10455585\n",
            "Iteration 453, loss = 0.10484882\n",
            "Iteration 454, loss = 0.10464329\n",
            "Iteration 455, loss = 0.10442278\n",
            "Iteration 456, loss = 0.10368911\n",
            "Iteration 457, loss = 0.10291478\n",
            "Iteration 458, loss = 0.10231970\n",
            "Iteration 459, loss = 0.10228201\n",
            "Iteration 460, loss = 0.10205300\n",
            "Iteration 461, loss = 0.10211955\n",
            "Iteration 462, loss = 0.10224258\n",
            "Iteration 463, loss = 0.10214557\n",
            "Iteration 464, loss = 0.10184883\n",
            "Iteration 465, loss = 0.10140384\n",
            "Iteration 466, loss = 0.10102830\n",
            "Iteration 467, loss = 0.10075679\n",
            "Iteration 468, loss = 0.10024316\n",
            "Iteration 469, loss = 0.09983642\n",
            "Iteration 470, loss = 0.09945861\n",
            "Iteration 471, loss = 0.09905811\n",
            "Iteration 472, loss = 0.09889967\n",
            "Iteration 473, loss = 0.09860489\n",
            "Iteration 474, loss = 0.09836654\n",
            "Iteration 475, loss = 0.09796639\n",
            "Iteration 476, loss = 0.09763706\n",
            "Iteration 477, loss = 0.09729682\n",
            "Iteration 478, loss = 0.09697771\n",
            "Iteration 479, loss = 0.09666154\n",
            "Iteration 480, loss = 0.09629444\n",
            "Iteration 481, loss = 0.09597406\n",
            "Iteration 482, loss = 0.09565907\n",
            "Iteration 483, loss = 0.09538868\n",
            "Iteration 484, loss = 0.09511059\n",
            "Iteration 485, loss = 0.09481059\n",
            "Iteration 486, loss = 0.09444170\n",
            "Iteration 487, loss = 0.09403969\n",
            "Iteration 488, loss = 0.09400104\n",
            "Iteration 489, loss = 0.09407855\n",
            "Iteration 490, loss = 0.09380617\n",
            "Iteration 491, loss = 0.09341578\n",
            "Iteration 492, loss = 0.09310305\n",
            "Iteration 493, loss = 0.09278799\n",
            "Iteration 494, loss = 0.09251050\n",
            "Iteration 495, loss = 0.09205850\n",
            "Iteration 496, loss = 0.09191312\n",
            "Iteration 497, loss = 0.09151426\n",
            "Iteration 498, loss = 0.09140427\n",
            "Iteration 499, loss = 0.09115299\n",
            "Iteration 500, loss = 0.09087104\n",
            "Iteration 501, loss = 0.09053846\n",
            "Iteration 502, loss = 0.09039619\n",
            "Iteration 503, loss = 0.09011592\n",
            "Iteration 504, loss = 0.08988037\n",
            "Iteration 505, loss = 0.08967902\n",
            "Iteration 506, loss = 0.08962599\n",
            "Iteration 507, loss = 0.08924577\n",
            "Iteration 508, loss = 0.08904015\n",
            "Iteration 509, loss = 0.08912468\n",
            "Iteration 510, loss = 0.08891704\n",
            "Iteration 511, loss = 0.08853385\n",
            "Iteration 512, loss = 0.08824959\n",
            "Iteration 513, loss = 0.08797114\n",
            "Iteration 514, loss = 0.08791976\n",
            "Iteration 515, loss = 0.08801404\n",
            "Iteration 516, loss = 0.08763135\n",
            "Iteration 517, loss = 0.08711063\n",
            "Iteration 518, loss = 0.08665666\n",
            "Iteration 519, loss = 0.08661632\n",
            "Iteration 520, loss = 0.08632719\n",
            "Iteration 521, loss = 0.08615394\n",
            "Iteration 522, loss = 0.08591089\n",
            "Iteration 523, loss = 0.08571254\n",
            "Iteration 524, loss = 0.08536997\n",
            "Iteration 525, loss = 0.08533860\n",
            "Iteration 526, loss = 0.08517389\n",
            "Iteration 527, loss = 0.08497893\n",
            "Iteration 528, loss = 0.08476509\n",
            "Iteration 529, loss = 0.08442403\n",
            "Iteration 530, loss = 0.08419365\n",
            "Iteration 531, loss = 0.08378491\n",
            "Iteration 532, loss = 0.08384225\n",
            "Iteration 533, loss = 0.08341583\n",
            "Iteration 534, loss = 0.08307679\n",
            "Iteration 535, loss = 0.08281059\n",
            "Iteration 536, loss = 0.08270517\n",
            "Iteration 537, loss = 0.08237000\n",
            "Iteration 538, loss = 0.08209550\n",
            "Iteration 539, loss = 0.08189858\n",
            "Iteration 540, loss = 0.08206219\n",
            "Iteration 541, loss = 0.08176664\n",
            "Iteration 542, loss = 0.08150769\n",
            "Iteration 543, loss = 0.08137811\n",
            "Iteration 544, loss = 0.08117019\n",
            "Iteration 545, loss = 0.08110420\n",
            "Iteration 546, loss = 0.08100577\n",
            "Iteration 547, loss = 0.08090577\n",
            "Iteration 548, loss = 0.08059457\n",
            "Iteration 549, loss = 0.08024866\n",
            "Iteration 550, loss = 0.07998974\n",
            "Iteration 551, loss = 0.07966995\n",
            "Iteration 552, loss = 0.07955945\n",
            "Iteration 553, loss = 0.07930095\n",
            "Iteration 554, loss = 0.07914685\n",
            "Iteration 555, loss = 0.07904441\n",
            "Iteration 556, loss = 0.07888572\n",
            "Iteration 557, loss = 0.07873870\n",
            "Iteration 558, loss = 0.07855063\n",
            "Iteration 559, loss = 0.07853706\n",
            "Iteration 560, loss = 0.07833102\n",
            "Iteration 561, loss = 0.07810071\n",
            "Iteration 562, loss = 0.07801685\n",
            "Iteration 563, loss = 0.07780290\n",
            "Iteration 564, loss = 0.07752509\n",
            "Iteration 565, loss = 0.07721854\n",
            "Iteration 566, loss = 0.07702249\n",
            "Iteration 567, loss = 0.07679611\n",
            "Iteration 568, loss = 0.07656022\n",
            "Iteration 569, loss = 0.07647500\n",
            "Iteration 570, loss = 0.07629110\n",
            "Iteration 571, loss = 0.07627618\n",
            "Iteration 572, loss = 0.07599237\n",
            "Iteration 573, loss = 0.07573661\n",
            "Iteration 574, loss = 0.07544201\n",
            "Iteration 575, loss = 0.07528036\n",
            "Iteration 576, loss = 0.07503840\n",
            "Iteration 577, loss = 0.07471061\n",
            "Iteration 578, loss = 0.07455351\n",
            "Iteration 579, loss = 0.07453341\n",
            "Iteration 580, loss = 0.07436091\n",
            "Iteration 581, loss = 0.07440611\n",
            "Iteration 582, loss = 0.07435270\n",
            "Iteration 583, loss = 0.07428577\n",
            "Iteration 584, loss = 0.07421716\n",
            "Iteration 585, loss = 0.07432396\n",
            "Iteration 586, loss = 0.07435896\n",
            "Iteration 587, loss = 0.07420915\n",
            "Iteration 588, loss = 0.07404886\n",
            "Iteration 589, loss = 0.07397655\n",
            "Iteration 590, loss = 0.07381296\n",
            "Iteration 591, loss = 0.07364306\n",
            "Iteration 592, loss = 0.07330288\n",
            "Iteration 593, loss = 0.07295123\n",
            "Iteration 594, loss = 0.07275579\n",
            "Iteration 595, loss = 0.07265252\n",
            "Iteration 596, loss = 0.07280645\n",
            "Iteration 597, loss = 0.07247989\n",
            "Iteration 598, loss = 0.07206561\n",
            "Iteration 599, loss = 0.07154024\n",
            "Iteration 600, loss = 0.07131509\n",
            "Iteration 601, loss = 0.07116770\n",
            "Iteration 602, loss = 0.07121026\n",
            "Iteration 603, loss = 0.07098100\n",
            "Iteration 604, loss = 0.07071343\n",
            "Iteration 605, loss = 0.07042278\n",
            "Iteration 606, loss = 0.07045006\n",
            "Iteration 607, loss = 0.07042967\n",
            "Iteration 608, loss = 0.07021364\n",
            "Iteration 609, loss = 0.06995985\n",
            "Iteration 610, loss = 0.06970549\n",
            "Iteration 611, loss = 0.06950991\n",
            "Iteration 612, loss = 0.06931244\n",
            "Iteration 613, loss = 0.06918467\n",
            "Iteration 614, loss = 0.06898058\n",
            "Iteration 615, loss = 0.06881126\n",
            "Iteration 616, loss = 0.06857822\n",
            "Iteration 617, loss = 0.06872726\n",
            "Iteration 618, loss = 0.06840986\n",
            "Iteration 619, loss = 0.06821736\n",
            "Iteration 620, loss = 0.06806769\n",
            "Iteration 621, loss = 0.06799023\n",
            "Iteration 622, loss = 0.06802018\n",
            "Iteration 623, loss = 0.06806124\n",
            "Iteration 624, loss = 0.06794472\n",
            "Iteration 625, loss = 0.06778071\n",
            "Iteration 626, loss = 0.06785425\n",
            "Iteration 627, loss = 0.06794979\n",
            "Iteration 628, loss = 0.06776049\n",
            "Iteration 629, loss = 0.06737333\n",
            "Iteration 630, loss = 0.06692546\n",
            "Iteration 631, loss = 0.06684537\n",
            "Iteration 632, loss = 0.06706200\n",
            "Iteration 633, loss = 0.06713765\n",
            "Iteration 634, loss = 0.06696477\n",
            "Iteration 635, loss = 0.06658867\n",
            "Iteration 636, loss = 0.06642088\n",
            "Iteration 637, loss = 0.06601358\n",
            "Iteration 638, loss = 0.06591898\n",
            "Iteration 639, loss = 0.06580551\n",
            "Iteration 640, loss = 0.06581902\n",
            "Iteration 641, loss = 0.06582050\n",
            "Iteration 642, loss = 0.06576730\n",
            "Iteration 643, loss = 0.06562235\n",
            "Iteration 644, loss = 0.06534133\n",
            "Iteration 645, loss = 0.06521758\n",
            "Iteration 646, loss = 0.06521601\n",
            "Iteration 647, loss = 0.06564598\n",
            "Iteration 648, loss = 0.06530833\n",
            "Iteration 649, loss = 0.06481911\n",
            "Iteration 650, loss = 0.06463238\n",
            "Iteration 651, loss = 0.06453546\n",
            "Iteration 652, loss = 0.06438134\n",
            "Iteration 653, loss = 0.06439973\n",
            "Iteration 654, loss = 0.06427773\n",
            "Iteration 655, loss = 0.06407467\n",
            "Iteration 656, loss = 0.06392891\n",
            "Iteration 657, loss = 0.06374104\n",
            "Iteration 658, loss = 0.06370138\n",
            "Iteration 659, loss = 0.06348328\n",
            "Iteration 660, loss = 0.06356460\n",
            "Iteration 661, loss = 0.06369383\n",
            "Iteration 662, loss = 0.06409021\n",
            "Iteration 663, loss = 0.06421246\n",
            "Iteration 664, loss = 0.06395914\n",
            "Iteration 665, loss = 0.06340288\n",
            "Iteration 666, loss = 0.06272084\n",
            "Iteration 667, loss = 0.06242057\n",
            "Iteration 668, loss = 0.06206507\n",
            "Iteration 669, loss = 0.06186233\n",
            "Iteration 670, loss = 0.06175101\n",
            "Iteration 671, loss = 0.06172366\n",
            "Iteration 672, loss = 0.06149730\n",
            "Iteration 673, loss = 0.06145016\n",
            "Iteration 674, loss = 0.06122838\n",
            "Iteration 675, loss = 0.06118265\n",
            "Iteration 676, loss = 0.06117995\n",
            "Iteration 677, loss = 0.06113368\n",
            "Iteration 678, loss = 0.06107867\n",
            "Iteration 679, loss = 0.06102724\n",
            "Iteration 680, loss = 0.06111340\n",
            "Iteration 681, loss = 0.06120455\n",
            "Iteration 682, loss = 0.06129542\n",
            "Iteration 683, loss = 0.06123423\n",
            "Iteration 684, loss = 0.06132538\n",
            "Iteration 685, loss = 0.06103366\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Vamos testar o modelo?**\n",
        "\n"
      ],
      "metadata": {
        "id": "Oq-S4o3IczVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = modelo.predict(X_teste)"
      ],
      "metadata": {
        "id": "1q9nsbSjdu23"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes"
      ],
      "metadata": {
        "id": "D0PlSJE8fAUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501c2371-84dd-4c64-f216-be2b03707888"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events'], dtype='<U20')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Será se o modelo acertou?**\n",
        "\n"
      ],
      "metadata": {
        "id": "FjWziqc5fV8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_teste"
      ],
      "metadata": {
        "id": "Q92H3KOtfN5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864bf803-aaec-436b-ee87-2bd4956fc72e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy_score(y_teste,previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ9MxYOIfmwv",
        "outputId": "d81573e1-45aa-4ea6-d922-c25e1a623343"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6724137931034483"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "confusion_matrix(y_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3D5bvushr9W",
        "outputId": "45328eb3-d2be-4d4c-c5e1-248e262d7d9e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30,  6],\n",
              "       [13,  9]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(modelo)\n",
        "cm.fit(X_treino, y_treino)\n",
        "cm.score(X_teste, y_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "wX15YT-7j-c9",
        "outputId": "db8b53f9-e72b-4257-c955-a229ab056d41"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6724137931034483"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_teste, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIixOPw1kw-z",
        "outputId": "5b4735c0-0e3f-4981-b422-f4a5d25f5c76"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.70      0.83      0.76        36\n",
            "   recurrence-events       0.60      0.41      0.49        22\n",
            "\n",
            "            accuracy                           0.67        58\n",
            "           macro avg       0.65      0.62      0.62        58\n",
            "        weighted avg       0.66      0.67      0.66        58\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Veja como implementar o backpropagation em python:**\n",
        "https://www.askpython.com/python/examples/backpropagation-in-python\n",
        "https://www.deeplearningbook.com.br/algoritmo-backpropagation-em-python/\n"
      ],
      "metadata": {
        "id": "csRtFY7lKr0N"
      }
    }
  ]
}