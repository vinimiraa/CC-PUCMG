{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Inteligência Artificial**\n",
        "\n",
        "**812839 - Vinícius Miranda de Araújo**\n",
        "\n",
        "**Lista 04**\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "nKdE4_Bk2vsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Questão 1**\n",
        "\n",
        "---\n",
        "\n",
        "Utilizando o **BayesSearchCV** e ajustando os hiperparâmetros do **Random Forest** e **Árvore de decisão** para o problema do TITANIC."
      ],
      "metadata": {
        "id": "EYrt4f_B2_md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from sklearn import tree\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "# Ler o arquivo de treino\n",
        "training_data = pd.read_csv( 'titanic/train.csv' )\n",
        "# Ler o arquivo de teste\n",
        "test_data = pd.read_csv( 'titanic/test.csv' )\n",
        "# Ler o arquivo com a resposta correta do o conjunto de teste\n",
        "truth_table = pd.read_csv( 'titanic/gender_submission.csv' )\n",
        "\n",
        "# Adicionar coluna 'Survived' ao test_data\n",
        "test_data = test_data.merge(truth_table, on='PassengerId', how='left')\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Pre-processamentos de Dados\n",
        "# ------------------------------------\n",
        "\n",
        "# Remover colunas irrelevantes ou com muitos valores ausentes\n",
        "columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n",
        "\n",
        "# Transformação de dados categóricos\n",
        "encoder = LabelEncoder()\n",
        "training_data['Sex'] = encoder.fit_transform( training_data['Sex'] )\n",
        "test_data['Sex'] = encoder.transform( test_data['Sex'] )\n",
        "\n",
        "# Preenchendo valores ausentes\n",
        "training_data['Age'] = training_data['Age'].fillna( training_data['Age'].median( ) )\n",
        "test_data['Age'] = test_data['Age'].fillna( test_data['Age'].median( ) )\n",
        "\n",
        "training_data['Fare'] = training_data['Fare'].fillna( training_data['Fare'].median( ) )\n",
        "test_data['Fare'] = test_data['Fare'].fillna( test_data['Fare'].median( ) )\n",
        "\n",
        "# Separar variáveis independentes e dependentes\n",
        "X_treino = training_data.drop( columns = columns_to_drop + ['Survived'], axis = 1 ) # X_treino = colunas de treino\n",
        "y_treino = training_data['Survived']                                                # y_treino = coluna de resposta\n",
        "\n",
        "X_teste = test_data.drop( columns = columns_to_drop + ['Survived'], axis = 1 )  # X_teste = colunas de teste\n",
        "y_teste = test_data['Survived']                                                 # y_teste = coluna de resposta\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Descobrir melhores hiperparâmetros\n",
        "# ------------------------------------\n",
        "\n",
        "# Hiperparâmetros Decision Tree\n",
        "dt_params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 2, 4, 6, 8, 10],\n",
        "    'max_features': [None, 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8],\n",
        "    'min_samples_split': [20, 30, 40, 50]\n",
        "}\n",
        "\n",
        "# Hiperparâmetros Random Forest\n",
        "rf_params = {\n",
        "    'n_estimators': (50, 200),\n",
        "    'max_depth': (2, 10),\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'min_samples_split': (2, 10)\n",
        "}\n",
        "\n",
        "# Encontrar melhores hiperparâmetros\n",
        "dt_modelo = BayesSearchCV(\n",
        "    DecisionTreeClassifier( ),\n",
        "    search_spaces = dt_params,\n",
        "    cv = 5,\n",
        "    n_jobs  = -1,\n",
        "    verbose = 1\n",
        ")\n",
        "dt_modelo.fit( X_treino, y_treino )\n",
        "print( \"Melhores hiperparâmetros   - Decision Tree:\", dt_modelo.best_params_ )\n",
        "\n",
        "# Otimização Random Forest\n",
        "rf_modelo = BayesSearchCV(\n",
        "    RandomForestClassifier( ),\n",
        "    search_spaces = rf_params,\n",
        "    cv = 5,\n",
        "    n_jobs = -1,\n",
        "    verbose = 1\n",
        ")\n",
        "rf_modelo.fit( X_treino, y_treino )\n",
        "print( \"Melhores hiperparâmetros   - Random Forest:\", rf_modelo.best_params_ )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Treinar o Modelo\n",
        "# ------------------------------------\n",
        "\n",
        "# Treinar modelo final com os melhores hiperparâmetros\n",
        "dt_modelo_final = DecisionTreeClassifier( **dt_modelo.best_params_ )\n",
        "dt_modelo_final.fit( X_treino, y_treino )\n",
        "\n",
        "rf_modelo_final = RandomForestClassifier( **rf_modelo.best_params_ )\n",
        "rf_modelo_final.fit( X_treino, y_treino )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Testar e Avaliar o Modelo\n",
        "# ------------------------------------\n",
        "\n",
        "# Fazer previsões\n",
        "dt_pred = dt_modelo_final.predict( X_teste )\n",
        "rf_pred = rf_modelo_final.predict( X_teste )\n",
        "\n",
        "# Avaliar o modelo\n",
        "print( \"Acurácia do modelo         - DecisionTree:\"  , accuracy_score( y_teste, dt_pred ) )\n",
        "print( \"Matriz de Confusão         - DecisionTree:\\n\", confusion_matrix( y_teste, dt_pred ) )\n",
        "print( \"Relatório de Classificação - DecisionTree:\\n\", classification_report( y_teste, dt_pred ) )\n",
        "\n",
        "print( \"Acurácia do modelo         - RandomForest:\"  , accuracy_score( y_teste, rf_pred ) )\n",
        "print( \"Matriz de Confusão         - RandomForest:\\n\", confusion_matrix( y_teste, rf_pred ) )\n",
        "print( \"Relatório de Classificação - RandomForest:\\n\", classification_report( y_teste, rf_pred ) )\n",
        "\n",
        "# Importância das features\n",
        "def plot_feature_importance( modelo, X_treino ):\n",
        "    importancias = modelo.feature_importances_\n",
        "    features     = pd.DataFrame( {'Feature': X_treino.columns, 'Importância': importancias} )\n",
        "    features     = features.sort_values( by='Importância', ascending=False )\n",
        "    print( features )\n",
        "# plot_feature_importance( )\n",
        "\n",
        "print( \"Importância dos Atributos - Decision Tree\" )\n",
        "plot_feature_importance( dt_modelo_final, X_treino )\n",
        "\n",
        "print( \"\\nImportância dos Atributos - Random Forest\" )\n",
        "plot_feature_importance( rf_modelo_final, X_treino )"
      ],
      "metadata": {
        "id": "Abibwgoi3bqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Avaliação de Desempenho**\n",
        "\n",
        "**Decision Tree**:\n",
        "\n",
        "- Acurácia:\n",
        "\n",
        "> 0.8588 = 85.88%\n",
        "\n",
        "- Classificação:\n",
        "\n",
        "> |       | **precision** | **recall** | **f1-score** |\n",
        "> |-------|---------------|------------|--------------|\n",
        "> | **0** |    0.88       |  0.90      |   0.89       |\n",
        "> | **1** |    0.82       |  0.78      |   0.80       |\n",
        "\n",
        "**Random Forest**:\n",
        "\n",
        "- Acurácia:\n",
        "\n",
        "> 0.8851 = 88.51%\n",
        "\n",
        "- Classificação:\n",
        "\n",
        "> |       | **precision** | **recall** | **f1-score** |\n",
        "> |-------|---------------|------------|--------------|\n",
        "> | **0** |    0.89       |  0.93      |   0.91       |\n",
        "> | **1** |    0.87       |  0.80      |   0.84       |\n",
        "\n",
        "#### **Conclusão**\n",
        "\n",
        "O **Random Forest** obteve melhor desempenho, com **acurácia de 88.5%** contra **85.9% da Decision Tree**.\n",
        "\n",
        "O **Random Forest** teve melhor recall para a classe 0 (não sobreviveu) e maior equilíbrio entre precisão e recall para a classe 1 (sobreviveu).\n"
      ],
      "metadata": {
        "id": "czW9401-4NS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Avaliação de Atributos**\n",
        "\n",
        "Os **quatro atributos principais são os mesmos** em ambos os modelos, mas com **importâncias ligeiramente diferentes**:  \n",
        "\n",
        "| **Feature** | **Decision Tree** | **Random Forest** |\n",
        "|-------------|-------------------|-------------------|\n",
        "| **Sex**     | 39.0%             | 40.0%             |\n",
        "| **Fare**    | 26.9%             | 21.8%             |\n",
        "| **Age**     | 18.2%             | 17.5%             |\n",
        "| **Pclass**  | 12.8%             | 12.1%             |\n",
        "| **SibSp**   | 1.9%              | 4.8%              |\n",
        "| **Parch**   | 0.9%              | 3.6%              |\n",
        "\n",
        "#### **Conclusão**\n",
        "  \n",
        "- **Sexo** é a feature mais relevante nos dois modelos.  \n",
        "- **Fare (Tarifa paga pelo passageiro)** tem mais peso na Decision Tree do que no Random Forest.  \n",
        "- **SibSp e Parch** têm pouca importância, mas no Random Forest sua relevância é maior.  \n"
      ],
      "metadata": {
        "id": "eMOPAUPN6riW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Questão 2**\n",
        "\n",
        "---\n",
        "\n",
        "Utilizando o **SMOTE** para balanceamento dos dados:"
      ],
      "metadata": {
        "id": "bGnrZ11V_7fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn import tree\n",
        "\n",
        "# Ler o arquivo de treino\n",
        "training_data = pd.read_csv( 'titanic/train.csv' )\n",
        "# Ler o arquivo de teste\n",
        "test_data = pd.read_csv( 'titanic/test.csv' )\n",
        "# Ler o arquivo com a resposta correta do o conjunto de teste\n",
        "truth_table = pd.read_csv( 'titanic/gender_submission.csv' )\n",
        "\n",
        "# Adicionar coluna 'Survived' ao test_data\n",
        "test_data = test_data.merge(truth_table, on='PassengerId', how='left')\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Pre-processamentos de Dados\n",
        "# ------------------------------------\n",
        "\n",
        "# Remover colunas irrelevantes ou com muitos valores ausentes\n",
        "columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n",
        "\n",
        "# Transformação de dados categóricos\n",
        "encoder = LabelEncoder()\n",
        "training_data['Sex'] = encoder.fit_transform( training_data['Sex'] )\n",
        "test_data['Sex'] = encoder.transform( test_data['Sex'] )\n",
        "\n",
        "# Preenchendo valores ausentes\n",
        "training_data['Age'] = training_data['Age'].fillna( training_data['Age'].median( ) )\n",
        "test_data['Age'] = test_data['Age'].fillna( test_data['Age'].median( ) )\n",
        "\n",
        "training_data['Fare'] = training_data['Fare'].fillna( training_data['Fare'].median( ) )\n",
        "test_data['Fare'] = test_data['Fare'].fillna( test_data['Fare'].median( ) )\n",
        "\n",
        "# Separar variáveis independentes e dependentes\n",
        "X_treino = training_data.drop( columns = columns_to_drop + ['Survived'], axis = 1 ) # X_treino = colunas de treino\n",
        "y_treino = training_data['Survived']                                                # y_treino = coluna de resposta\n",
        "\n",
        "X_teste = test_data.drop( columns = columns_to_drop + ['Survived'], axis = 1 ) # X_teste = colunas de teste\n",
        "y_teste = test_data['Survived']                                                # y_teste = coluna de resposta\n",
        "\n",
        "# Balenceamento dos dados com SMOTE\n",
        "smote = SMOTE( random_state = 42 )\n",
        "X_resampled, y_resampled = smote.fit_resample( X_treino, y_treino )\n",
        "\n",
        "print( \"Antes do balanceamento  - X:\\n\", X_treino.value_counts( ) )\n",
        "print( \"Depois do balanceamento - X:\\n\", X_resampled.value_counts( ) )\n",
        "\n",
        "print( \"Antes do balanceamento  - y:\\n\", y_treino.value_counts( ) )\n",
        "print( \"Depois do balanceamento - y:\\n\", y_resampled.value_counts( ) )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Descobrir melhores hiperparâmetros\n",
        "# ------------------------------------\n",
        "\n",
        "# Hiperparâmetros Random Forest\n",
        "rf_params = {\n",
        "    'n_estimators': (50, 200),\n",
        "    'max_depth': (2, 10),\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'min_samples_split': (2, 10)\n",
        "}\n",
        "\n",
        "# Otimização Random Forest\n",
        "rf_modelo = BayesSearchCV(\n",
        "    RandomForestClassifier( ),\n",
        "    search_spaces = rf_params,\n",
        "    cv = 5,\n",
        "    n_jobs = -1,\n",
        "    verbose = 1,\n",
        "    random_state = 42\n",
        ")\n",
        "rf_modelo.fit( X_resampled, y_resampled )\n",
        "print( \"Melhores hiperparâmetros   - Random Forest:\", rf_modelo.best_params_ )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Treinar o Modelo\n",
        "# ------------------------------------\n",
        "\n",
        "# Treinar modelo final com os melhores hiperparâmetros\n",
        "rf_modelo_final = RandomForestClassifier( **rf_modelo.best_params_, random_state = 42 )\n",
        "rf_modelo_final.fit( X_resampled, y_resampled )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Testar e Avaliar o Modelo\n",
        "# ------------------------------------\n",
        "\n",
        "# Fazer previsões\n",
        "rf_pred = rf_modelo_final.predict( X_teste )\n",
        "\n",
        "# Avaliar o modelo\n",
        "print( \"Acurácia do modelo         - RandomForest:\"  , accuracy_score( y_teste, rf_pred ) )\n",
        "print( \"Matriz de Confusão         - RandomForest:\\n\", confusion_matrix( y_teste, rf_pred ) )\n",
        "print( \"Relatório de Classificação - RandomForest:\\n\", classification_report( y_teste, rf_pred ) )\n",
        "\n",
        "# Importância das features\n",
        "def plot_feature_importance( modelo, X_treino ):\n",
        "    importancias = modelo.feature_importances_\n",
        "    features     = pd.DataFrame( {'Feature': X_treino.columns, 'Importância': importancias} )\n",
        "    features     = features.sort_values( by='Importância', ascending=False )\n",
        "    print( features )\n",
        "# plot_feature_importance( )\n",
        "\n",
        "print( \"\\nImportância dos Atributos - Random Forest\" )\n",
        "plot_feature_importance( rf_modelo_final, X_resampled )"
      ],
      "metadata": {
        "id": "ituwuhu_Aoyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Avaliação**\n",
        "\n",
        "**ANTES**:\n",
        "\n",
        "- Acurácia:\n",
        "\n",
        "> 0.8851 = 88.51%\n",
        "\n",
        "- Classificação:\n",
        "\n",
        "> |       | **precision** | **recall** | **f1-score** |\n",
        "> |-------|---------------|------------|--------------|\n",
        "> | **0** |    0.89       |  0.93      |   0.91       |\n",
        "> | **1** |    0.87       |  0.80      |   0.84       |\n",
        "\n",
        "**DEPOIS**:\n",
        "\n",
        "- Acurácia:\n",
        "\n",
        "> 0.8660 = 86.6%\n",
        "\n",
        "- Classificação:\n",
        "\n",
        "> |       | **precision** | **recall** | **f1-score** |\n",
        "> |-------|---------------|------------|--------------|\n",
        "> | **0** |    0.89       |  0.91      |   0.90       |\n",
        "> | **1** |    0.83       |  0.80      |   0.81       |\n",
        "\n",
        "#### **Conclusão**\n",
        "\n",
        "A principal mudança após o balanceamento com SMOTE foi:\n",
        "\n",
        "- Leve queda na acurácia (de 0.89 para 0.87).\n",
        "- F1-score da classe 1 (sobreviventes) caiu um pouco (0.84 → 0.81).\n",
        "- Os pesos das variáveis mudaram levemente, mas Sex, Fare e Age continuam sendo os atributos mais importantes."
      ],
      "metadata": {
        "id": "Vgzz4BbJBHA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Questão 3**\n",
        "\n",
        "---\n",
        "\n",
        "Antes, eu estava imputando os valores ausentes utilizando a **mediana** para preencher os campos vazios. Esse método é simples e eficaz para evitar a perda de dados, mas não leva em consideração as relações entre diferentes variáveis.\n",
        "\n",
        "Agora, vou utilizar o **KNNImputer**, que utiliza a média dos k vizinhos mais próximos para preencher os valores ausentes de forma mais inteligente."
      ],
      "metadata": {
        "id": "tgWjB4CoF5_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn import tree\n",
        "\n",
        "# Ler o arquivo de treino\n",
        "training_data = pd.read_csv( 'titanic/train.csv' )\n",
        "# Ler o arquivo de teste\n",
        "test_data = pd.read_csv( 'titanic/test.csv' )\n",
        "# Ler o arquivo com a resposta correta do o conjunto de teste\n",
        "truth_table = pd.read_csv( 'titanic/gender_submission.csv' )\n",
        "\n",
        "# Adicionar coluna 'Survived' ao test_data\n",
        "test_data = test_data.merge(truth_table, on='PassengerId', how='left')\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Pre-processamentos de Dados\n",
        "# ------------------------------------\n",
        "\n",
        "# Remover colunas irrelevantes ou com muitos valores ausentes\n",
        "columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\n",
        "\n",
        "# Transformação de dados categóricos\n",
        "encoder = LabelEncoder()\n",
        "training_data['Sex'] = encoder.fit_transform( training_data['Sex'] )\n",
        "test_data['Sex'] = encoder.transform( test_data['Sex'] )\n",
        "\n",
        "# Preenchendo valores ausentes com KNNImputer\n",
        "# training_data['Age'] = training_data['Age'].fillna(training_data['Age'].median())\n",
        "# test_data['Age'] = test_data['Age'].fillna(test_data['Age'].median())\n",
        "\n",
        "# training_data['Fare'] = training_data['Fare'].fillna(training_data['Fare'].median())\n",
        "# test_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n",
        "\n",
        "imputer = KNNImputer( n_neighbors=5 )\n",
        "columns_to_input = ['Age', 'Fare']\n",
        "training_data[columns_to_input] = imputer.fit_transform( training_data[columns_to_input] )\n",
        "test_data[columns_to_input] = imputer.transform( test_data[columns_to_input] )\n",
        "\n",
        "# Separar variáveis independentes e dependentes\n",
        "X_treino = training_data.drop( columns = columns_to_drop + ['Survived'], axis = 1 ) # X_treino = colunas de treino\n",
        "y_treino = training_data['Survived']                                                # y_treino = coluna de resposta\n",
        "\n",
        "X_teste = test_data.drop( columns = columns_to_drop + ['Survived'], axis = 1 ) # X_teste = colunas de teste\n",
        "y_teste = test_data['Survived']                                                # y_teste = coluna de resposta\n",
        "\n",
        "# Balenceamento dos dados com SMOTE\n",
        "smote = SMOTE( random_state = 42 )\n",
        "X_resampled, y_resampled = smote.fit_resample( X_treino, y_treino )\n",
        "\n",
        "print( \"Antes do balanceamento  - X:\\n\", X_treino.value_counts( ) )\n",
        "print( \"Depois do balanceamento - X:\\n\", X_resampled.value_counts( ) )\n",
        "\n",
        "print( \"Antes do balanceamento  - y:\\n\", y_treino.value_counts( ) )\n",
        "print( \"Depois do balanceamento - y:\\n\", y_resampled.value_counts( ) )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Descobrir melhores hiperparâmetros\n",
        "# ------------------------------------\n",
        "\n",
        "# Hiperparâmetros Random Forest\n",
        "rf_params = {\n",
        "    'n_estimators': (50, 200),\n",
        "    'max_depth': (2, 10),\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'min_samples_split': (2, 10)\n",
        "}\n",
        "\n",
        "# Otimização Random Forest\n",
        "rf_modelo = BayesSearchCV(\n",
        "    RandomForestClassifier( ),\n",
        "    search_spaces = rf_params,\n",
        "    cv = 5,\n",
        "    n_jobs = -1,\n",
        "    verbose = 1,\n",
        "    random_state = 42\n",
        ")\n",
        "rf_modelo.fit( X_resampled, y_resampled )\n",
        "print( \"Melhores hiperparâmetros   - Random Forest:\", rf_modelo.best_params_ )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Treinar o Modelo\n",
        "# ------------------------------------\n",
        "\n",
        "# Treinar modelo final com os melhores hiperparâmetros\n",
        "rf_modelo_final = RandomForestClassifier( **rf_modelo.best_params_, random_state = 42 )\n",
        "rf_modelo_final.fit( X_resampled, y_resampled )\n",
        "\n",
        "# ------------------------------------\n",
        "# --- Testar e Avaliar o Modelo\n",
        "# ------------------------------------\n",
        "\n",
        "# Fazer previsões\n",
        "rf_pred = rf_modelo_final.predict( X_teste )\n",
        "\n",
        "# Avaliar o modelo\n",
        "print( \"Acurácia do modelo         - RandomForest:\"  , accuracy_score( y_teste, rf_pred ) )\n",
        "print( \"Matriz de Confusão         - RandomForest:\\n\", confusion_matrix( y_teste, rf_pred ) )\n",
        "print( \"Relatório de Classificação - RandomForest:\\n\", classification_report( y_teste, rf_pred ) )\n",
        "\n",
        "# Matriz de Confusão\n",
        "def plot_confusion_matrix( modelo, X_treino, y_treino, X_teste, y_teste ):\n",
        "    cm = ConfusionMatrix( modelo )\n",
        "    cm.fit( X_treino, y_treino )\n",
        "    cm.score( X_teste, y_teste )\n",
        "    cm.show( )\n",
        "# plot_confusion_matrix( )\n",
        "\n",
        "# Árvores de Decisão\n",
        "def plot_decision_tree( modelo, title ):\n",
        "    plt.figure( figsize=(15, 10) )\n",
        "    tree.plot_tree(\n",
        "        modelo,\n",
        "        feature_names = X_treino.columns,\n",
        "        class_names   = ['Não Sobreviveu', 'Sobreviveu'],\n",
        "        filled        = True,\n",
        "        rounded       = True\n",
        "    )\n",
        "    plt.title( title )\n",
        "    plt.show( )\n",
        "# plot_decision_tree( )\n",
        "\n",
        "# Importância das features\n",
        "def plot_feature_importance( modelo, X_treino ):\n",
        "    importancias = modelo.feature_importances_\n",
        "    features     = pd.DataFrame( {'Feature': X_treino.columns, 'Importância': importancias} )\n",
        "    features     = features.sort_values( by='Importância', ascending=False )\n",
        "    print( features )\n",
        "# plot_feature_importance( )\n",
        "\n",
        "print( \"\\nImportância dos Atributos - Random Forest\" )\n",
        "plot_feature_importance( rf_modelo_final, X_resampled )"
      ],
      "metadata": {
        "id": "3e9SwzCAGMXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Avaliação**\n",
        "\n",
        "**ANTES**\n",
        "\n",
        "- Melhores hiperparâmetros:\n",
        "\n",
        "      OrderedDict({'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 53})\n",
        "\n",
        "      Acurácia do modelo: 0.8660287081339713\n",
        "\n",
        "- Relatório de Classificação:\n",
        "\n",
        "                    precision    recall  f1-score   support\n",
        "\n",
        "                0       0.89      0.91      0.90       266\n",
        "                1       0.83      0.80      0.81       152\n",
        "\n",
        "          accuracy                          0.87       418\n",
        "         macro avg      0.86      0.85      0.85       418\n",
        "      weighted avg      0.87      0.87      0.87       418\n",
        "\n",
        "- Importância dos Atributos\n",
        "\n",
        "        Feature  Importância\n",
        "      1     Sex     0.382119\n",
        "      5    Fare     0.226022\n",
        "      2     Age     0.205113\n",
        "      0  Pclass     0.109001\n",
        "      3   SibSp     0.044947\n",
        "      4   Parch     0.032797\n",
        "\n",
        "**DEPOIS**\n",
        "\n",
        "- Melhores hiperparâmetros:\n",
        "\n",
        "      OrderedDict({'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200})\n",
        "\n",
        "      Acurácia do modelo: 0.8660287081339713\n",
        "\n",
        "- Relatório de Classificação:\n",
        "\n",
        "                    precision    recall  f1-score   support\n",
        "\n",
        "                0       0.88      0.91      0.90       266\n",
        "                1       0.83      0.79      0.81       152\n",
        "\n",
        "          accuracy                          0.87       418\n",
        "        macro avg       0.86      0.85      0.85       418\n",
        "      weighted avg      0.87      0.87      0.87       418\n",
        "\n",
        "- Importância dos Atributos:\n",
        "\n",
        "        Feature  Importância\n",
        "      1     Sex     0.377391\n",
        "      5    Fare     0.221382\n",
        "      2     Age     0.213991\n",
        "      0  Pclass     0.108402\n",
        "      3   SibSp     0.044182\n",
        "      4   Parch     0.034652\n",
        "\n",
        "#### **Conclusão**\n",
        "\n",
        "A substituição da mediana pelo **KNNImputer** **não trouxe ganho significativo na acurácia**. No entanto:\n",
        "\n",
        "- Houve uma pequena mudança na importância das variáveis.\n",
        "- A idade passou a ter um pouco mais de impacto no modelo.\n",
        "- A escolha dos hiperparâmetros foi diferente."
      ],
      "metadata": {
        "id": "_vijWvKTKLew"
      }
    }
  ]
}